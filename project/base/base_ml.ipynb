{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies:\n",
    "#  base_eq.ipynb\n",
    "#  base_plot.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sklearn.base as skb\n",
    "import sklearn.preprocessing as skpr\n",
    "import sklearn.pipeline as skpi\n",
    "import sklearn.linear_model as sklm\n",
    "\n",
    "# https://github.com/patrick-kidger/tools\n",
    "import tools\n",
    "\n",
    "import tensorflow as tf\n",
    "tfd = tf.data\n",
    "tfer = tf.errors\n",
    "tfe = tf.estimator\n",
    "tfi = tf.initializers\n",
    "tfk = tf.keras\n",
    "tfla = tf.layers\n",
    "tflog = tf.logging\n",
    "tflo = tf.losses\n",
    "tft = tf.train\n",
    "\n",
    "# Convenience imports for those files running this one\n",
    "import collections as co\n",
    "import functools as ft\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid hyperparameters\n",
    "# Everything we do is on a grid\n",
    "\n",
    "# The separation between points of the fine grid\n",
    "fine_grid_sep = tools.Object(t=0.01, x=0.01)\n",
    "# The separation between points of the coarse grid\n",
    "coarse_grid_sep = tools.Object(t=0.1, x=0.1)\n",
    "# The amount of intervals in the coarse grid. Thus the coarse grid will contain\n",
    "# (num_intervals.t + 1) * (num_intervals.x + 1) elements.\n",
    "# So with num_intervals.t = 3, num_intervals.x = 3, it looks like:\n",
    "#\n",
    "# @ @ @ @\n",
    "#\n",
    "# @ @ @ @\n",
    "#\n",
    "# @ @ @ @\n",
    "#\n",
    "# @ @ @ @\n",
    "num_intervals = tools.Object(t=7, x=7)\n",
    "\n",
    "\n",
    "fine_grid_fineness = tools.Object(t=int(coarse_grid_sep.t // fine_grid_sep.t), \n",
    "                                  x=int(coarse_grid_sep.x // fine_grid_sep.x))\n",
    "coarse_grid_size = tools.Object(t=num_intervals.t * coarse_grid_sep.t,\n",
    "                                x=num_intervals.x * coarse_grid_sep.x)\n",
    "\n",
    "\n",
    "### Grids to evaluate our solution on\n",
    "\n",
    "def grid(point, grid_size, grid_fineness):\n",
    "    \"\"\"Creates a grid whose bottom left entry is at the specified :point:\n",
    "    location. The size of the overall grid may be specified via :grid_size:, and\n",
    "    the fineness of the subdivision by :grid_fineness:, both of which should be\n",
    "    of the form tools.Object(t, x). Thus the resulting grid has\n",
    "    (grid_fineness.t + 1) * (grid_fineness.x + 1) elements.\"\"\"\n",
    "    t, x = point\n",
    "    return [(t_, x_) for t_ in np.linspace(t, t + grid_size.t, \n",
    "                                           grid_fineness.t + 1)\n",
    "                     for x_ in np.linspace(x, x + grid_size.x, \n",
    "                                           grid_fineness.x + 1)]\n",
    "\n",
    "def fine_grid(point):\n",
    "    \"\"\"Creates a fine grid whose bottom left entry is at the specified :point:\n",
    "    location, with size and fineness determined by the earlier hyperparameters.\n",
    "    \"\"\"\n",
    "    return grid(point, coarse_grid_sep, fine_grid_fineness)\n",
    "\n",
    "def coarse_grid(point):\n",
    "    \"\"\"Creates a coarse grid for which the bottom left entry of its middle\n",
    "    square is as the specified :t:, :x: location, with size and fineness\n",
    "    determined by the earlier hyperparameters.\n",
    "    \"\"\"\n",
    "    left_intervals_t = np.floor((num_intervals.t - 1) / 2)\n",
    "    left_intervals_x = np.floor((num_intervals.x - 1) / 2)\n",
    "    \n",
    "    left_amount_t = left_intervals_t * coarse_grid_sep.t\n",
    "    left_amount_x = left_intervals_x * coarse_grid_sep.x\n",
    "    \n",
    "    t, x = point\n",
    "    bottomleft_point = (t - left_amount_t, x - left_amount_x)\n",
    "    return grid(bottomleft_point, coarse_grid_size, num_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data generation\n",
    "\n",
    "def gen_one_peakon():\n",
    "    \"\"\"Returns a random peakon and a random location.\"\"\"\n",
    "    # Random solution to the CH equation\n",
    "    c = np.random.uniform(3, 10)\n",
    "    peakon = Peakon(c=c)\n",
    "    # Random location near the peak\n",
    "    t = np.random.uniform(0, 10)\n",
    "    x = c * t + np.random.uniform(-2, 2)\n",
    "    return (t, x), peakon\n",
    "\n",
    "def gen_two_peakon():\n",
    "    \"\"\"Returns a random two peakon solution, and a random location.\"\"\"\n",
    "    # Random solution to the CH equation\n",
    "    p1 = np.random.uniform(3, 10)\n",
    "    p2 = np.random.uniform(3, 10)\n",
    "    x1 = np.random.uniform(0, 3)\n",
    "    x2 = np.random.uniform(3.001, 6)\n",
    "    twopeakon = TwoPeakon(x1, x2, p1, p2)\n",
    "    # Random location near both of the peaks\n",
    "    t = np.random.uniform(0, 0.5)\n",
    "    left = min(x1 - 0.5 + p1 * t, x2 - 0.5 + p2 * t)\n",
    "    right = max(x1 + 0.5 + p1 * t, x2 + 0.5 + p2 * t)\n",
    "    middle = (right + left) / 2\n",
    "    semidist = (right - left) / 2\n",
    "    x = middle + semidist * np.random.uniform(-1, 1) ** 3\n",
    "    return (t, x), twopeakon\n",
    "\n",
    "def sol_on_grid(point, solution):\n",
    "    \"\"\"Returns the values of the :solution: on fine and coarse grids around the\n",
    "    specified :point:.\n",
    "    \"\"\"\n",
    "    # Grids at the location\n",
    "    cg = coarse_grid(point)\n",
    "    fg = fine_grid(point)\n",
    "    # Features: the solution on the coarse grid\n",
    "    X = solution.on_grid(cg)\n",
    "    # Labels: the solution on the fine grid\n",
    "    y = solution.on_grid(fg)\n",
    "    return X, y\n",
    "\n",
    "def sol_at_point(point, solution):\n",
    "    \"\"\"Returns the values of the :solution: on a coarse grid and at a random\n",
    "    point near the specified :point:.\n",
    "    \"\"\"\n",
    "    \n",
    "    cg = coarse_grid(point)\n",
    "    \n",
    "    # Random offset from the random location that we ask for predictions at. The\n",
    "    # distribution is asymmetric because we're moving relative to :point:, which\n",
    "    # is in the _bottom left_ of the central cell of the coarse grid. The asymmetric\n",
    "    # distribution thus makes this relative to te centre of the central cell.\n",
    "    #\n",
    "    # This value is not scaled relative to the size of the grid as we expect\n",
    "    # that the predictions should be scale invariant, and we do not want the\n",
    "    # network to unnecessarily learn the size of coarse_grid_sep.\n",
    "    x_offset = np.random.uniform(-0.5, 1.5)\n",
    "    t_offset = np.random.uniform(-0.5, 1.5)\n",
    "    \n",
    "    # Features: the solution on the coarse grid and the point to interpolate at.\n",
    "    X = solution.on_grid(cg, extra=2)\n",
    "    # We tell the network the offset; as the network has no way of knowing the\n",
    "    # location of the grid then adding a translation would only confuse it.\n",
    "    X[-2] = t_offset - 0.5  # -0.5 to normalise\n",
    "    X[-1] = x_offset - 0.5  # -0.5 to normalise\n",
    "    \n",
    "    t, x = point\n",
    "    # Label: the solution at the interpolation point\n",
    "    y = np.full(1, peakon((t + t_offset * coarse_grid_sep.t, \n",
    "                           x + x_offset * coarse_grid_sep.x)))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# A particularly nice X, y that is right on the peak of the peakon\n",
    "X_peak = np.array([0.71136994, 0.64367414, 0.58242045, 0.52699581, 0.47684553,\n",
    "                   0.43146768, 0.3904081 , 0.35325586, 1.53965685, 1.39313912,\n",
    "                   1.26056441, 1.14060584, 1.03206285, 0.93384908, 0.84498159,\n",
    "                   0.76457096, 3.33236346, 3.01524715, 2.72830845, 2.46867557,\n",
    "                   2.23375003, 2.02118061, 1.82883985, 1.65480272, 7.21241639,\n",
    "                   6.52606422, 5.9050271 , 5.34308947, 4.83462728, 4.37455167,\n",
    "                   3.95825804, 3.58157998, 3.81911647, 4.22077645, 4.66467938,\n",
    "                   5.155268  , 5.69745227, 6.29665855, 6.95888391, 7.69075612,\n",
    "                   1.76455206, 1.95013162, 2.15522875, 2.38189614, 2.63240234,\n",
    "                   2.90925451, 3.21522348, 3.55337148, 0.81527861, 0.90102221,\n",
    "                   0.99578354, 1.10051101, 1.21625277, 1.34416719, 1.48553448,\n",
    "                   1.64176951, 0.37668439, 0.41630063, 0.46008335, 0.50847074,\n",
    "                   0.56194707, 0.62104756, 0.68636371, 0.75854921])\n",
    "y_peak = np.array([5.34308947, 5.28992485, 5.23728921, 5.18517732, 5.13358394,\n",
    "                   5.08250393, 5.03193217, 4.98186361, 4.93229323, 4.8832161 ,\n",
    "                   4.83462728, 5.77198627, 5.71455405, 5.65769329, 5.6013983 ,\n",
    "                   5.54566345, 5.49048318, 5.43585196, 5.38176433, 5.32821488,\n",
    "                   5.27519826, 5.22270916, 6.23531118, 6.1732688 , 6.11184375,\n",
    "                   6.05102988, 5.99082113, 5.93121147, 5.87219493, 5.81376561,\n",
    "                   5.75591767, 5.69864534, 5.64194287, 6.73582778, 6.66880518,\n",
    "                   6.60244946, 6.53675399, 6.4717122 , 6.40731759, 6.34356371,\n",
    "                   6.2804442 , 6.21795273, 6.15608307, 6.09482902, 7.27652151,\n",
    "                   7.20411891, 7.13243673, 7.0614678 , 6.99120502, 6.92164137,\n",
    "                   6.85276989, 6.78458369, 6.71707595, 6.65023993, 6.58406894,\n",
    "                   7.58429925, 7.66052273, 7.70496678, 7.62830108, 7.55239822,\n",
    "                   7.4772506 , 7.40285071, 7.32919112, 7.25626445, 7.18406341,\n",
    "                   7.11258079, 7.0207356 , 7.09129517, 7.16256387, 7.23454883,\n",
    "                   7.30725726, 7.38069641, 7.45487364, 7.52979637, 7.60547208,\n",
    "                   7.68190835, 7.68351697, 6.49904846, 6.56436498, 6.63033795,\n",
    "                   6.69697395, 6.76427966, 6.8322618 , 6.90092717, 6.97028264,\n",
    "                   7.04033515, 7.11109169, 7.18255935, 6.01612613, 6.0765892 ,\n",
    "                   6.13765994, 6.19934444, 6.26164889, 6.32457951, 6.38814259,\n",
    "                   6.45234449, 6.51719163, 6.58269049, 6.64884763, 5.56908812,\n",
    "                   5.62505839, 5.68159116, 5.7386921 , 5.79636692, 5.85462137,\n",
    "                   5.9134613 , 5.97289257, 6.03292114, 6.093553  , 6.15479423,\n",
    "                   5.155268  , 5.2070793 , 5.25941132, 5.31226928, 5.36565848,\n",
    "                   5.41958424, 5.47405197, 5.5290671 , 5.58463515, 5.64076167,\n",
    "                   5.69745227])\n",
    "\n",
    "\n",
    "class BatchData:\n",
    "    \"\"\"Wrapper around tf.data.Dataset.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_func(gen_one_data, batch_size=1):\n",
    "        \"\"\"Takes a function :gen_one_data: which returns a generator and a\n",
    "        :batch_size:, and returns a function which returns a tf.data.Dataset \n",
    "        producing batches of that size.\n",
    "        \"\"\"\n",
    "        # Wrapper is because in order to be part of the same graph as the\n",
    "        # DNN, it has to be called later on.\n",
    "        def _wrapper():\n",
    "            def generator():\n",
    "                while True:\n",
    "                    yield gen_one_data()\n",
    "\n",
    "            X, y = gen_one_data()\n",
    "            ds = tfd.Dataset.from_generator(generator, \n",
    "                                            (X.dtype, y.dtype), \n",
    "                                            (tf.TensorShape(X.shape), \n",
    "                                             tf.TensorShape(y.shape)))\n",
    "            return ds.batch(batch_size)\n",
    "        return _wrapper\n",
    "\n",
    "    @staticmethod\n",
    "    def _from_data(data):\n",
    "        \"\"\"Returns a dataset which endlessly repeats :data:.\"\"\"\n",
    "        # Lambda wrapper is because in order to be part of the same graph as\n",
    "        # the DNN, it has to be called later on.\n",
    "        return lambda: tfd.Dataset.from_tensors(data).repeat()\n",
    "    \n",
    "    @classmethod\n",
    "    def from_single_data(cls, X, y):\n",
    "        \"\"\"Takes a single (feature, label) pair :X:, :y: and returns a\n",
    "        td.data.Dataset which endlessly produces that value.\n",
    "        \"\"\"\n",
    "        one_data = (np.array([X]), np.array([y]))\n",
    "        return cls._from_data(one_data)\n",
    "    \n",
    "    @classmethod\n",
    "    def test(cls, gen_one_data, batch_size=1):\n",
    "        \"\"\"Takes a function :gen_one_data: which returns a generator and a\n",
    "        :batch_size:, and returns a tf.data.Dataset which endlessly produces\n",
    "        one batch (always the same batch) of that size.\n",
    "        \"\"\"\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        for _ in range(batch_size):\n",
    "            X, y = gen_one_data()\n",
    "            X_data.append(X)\n",
    "            y_data.append(y)\n",
    "        data = (np.array(X_data), np.array(y_data))\n",
    "        return cls._from_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preprocessing\n",
    "\n",
    "class Processor(tools.SubclassTrackerMixin('__name__')):\n",
    "    \"\"\"Base class for preprocessors.\"\"\"\n",
    "    \n",
    "    save_attr = []\n",
    "    checkpoint_filename = 'processor-checkpoint'\n",
    "    \n",
    "    def __init__(self, training=True, **kwargs):\n",
    "        self.dataset = None\n",
    "        self._training = training\n",
    "        super(Processor, self).__init__(**kwargs)\n",
    "        self._saver = tft.Saver([getattr(self, name) for name in self.save_attr] allow_empty=True)\n",
    "        \n",
    "    def data(self, dataset):\n",
    "        \"\"\"Tells the processor where to get data from. The argument :dataset:\n",
    "        should be an instance of tf.data.Dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        return self  # for chaining\n",
    "    \n",
    "    def _dict_wrapper(self, X, y):\n",
    "        # For some mad reason TensorFlow seems to demand that the features be \n",
    "        # wrapped in a dictionary but that the labels are not.\n",
    "        return {'X': X}, y\n",
    "        \n",
    "    def __call__(self):\n",
    "        if self.dataset is None:\n",
    "            raise RuntimeError(\"Must be passed a tf.data.Dataset via 'data' \"\n",
    "                               \"method before being called.\")\n",
    "\n",
    "        ds = self.dataset()\n",
    "        return ds.map(self.transform).map(self._dict_wrapper)\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        \"\"\"Processes the data.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def inverse_transform(self, y):\n",
    "        \"\"\"Performs the inverse transform on the data.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def save(self, model_dir, session, step):\n",
    "        \"\"\"Saves the processor to a file in the directory :model_dir:. The argument\n",
    "        :step: is logged out to specify at what global step this was performed.\"\"\"\n",
    "        self._saver.save(session, model_dir + '/' + self.checkpoint_filename)\n",
    "        tflog.info('Saving processor checkpoint for {} into {}'.format(step, model_dir))\n",
    "        \n",
    "    def load(self, session, model_dir):\n",
    "        \"\"\"Sets the processor's variables to what is specified in the save file\n",
    "        located in the directory :model_dir:.\n",
    "        \"\"\"\n",
    "        self._saver.restore(session, model_dir + '/' + self.checkpoint_filename)\n",
    "            \n",
    "    \n",
    "    \n",
    "class IdentityProcessor(Processor):\n",
    "    \"\"\"Performs no processing, but still uses Processor so that the conversion\n",
    "    to TensorFlow (via the use_tf argument for __call__) may still be performed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        return X, y\n",
    "    \n",
    "    def inverse_transform(self, y):\n",
    "        return y\n",
    "  \n",
    "    \n",
    "class ScaleOverall(Processor):\n",
    "    \"\"\"Scales data to between -1 and 1. Scaling is done across all batches.\"\"\"\n",
    "    \n",
    "    save_attr = ['X_mean', 'X_extent', 'momentum', '_started']\n",
    "    \n",
    "    def __init__(self, momentum=0.99, **kwargs):\n",
    "        self.momentum = tf.Variable(momentum, trainable=False)\n",
    "        self.X_mean = tf.Variable(0.0, trainable=False, validate_shape=False)\n",
    "        self.X_extent = tf.Variable(0.0, trainable=False, validate_shape=False)\n",
    "        self._started = tf.Variable(False, trainable=False)\n",
    "        super(ScaleDataOverall, self).__init__(**kwargs)\n",
    "        \n",
    "    def transform(self, X, y):\n",
    "        def first_time():\n",
    "            self._started.assign(True)\n",
    "            X_mean = tf.reduce_mean(X)\n",
    "            X_extent = tf.reduce_max(tf.abs(X - X_mean))\n",
    "            self.X_mean.assign(X_mean)\n",
    "            self.X_extent.assign(X_extent)\n",
    "        \n",
    "        def later_times():\n",
    "            if self._training:\n",
    "                X_mean = tf.reduce_mean(X)\n",
    "                X_extent = tf.reduce_max(tf.abs(X - X_mean))\n",
    "                self.X_mean.assign(self.X_mean * self.momentum + X_mean * (1 - self.momentum))\n",
    "                self.X_extent.assign(self.X_extent * self.momentum + X_extent * (1 - self.momentum))\n",
    "        \n",
    "        tf.cond(tf.equal(self._started, False), first_time, later_times)\n",
    "        \n",
    "        X_scaled = (X - self.X_mean) / self.X_extent\n",
    "        y_scaled = (y - self.X_mean) / self.X_extent\n",
    "        return X_scaled, y_scaled\n",
    "    \n",
    "    def inverse_transform(self, y):\n",
    "        return (y * self.X_extent) + self.X_mean\n",
    "    \n",
    "    \n",
    "class NormalisationOverall(Processor):\n",
    "    \"\"\"Normalises inputs by subtracting mean and dividing by standard deviation.\n",
    "    Scaling is done across all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    save_attr = ['mean', 'stddev', 'momentum', '_started']\n",
    "    \n",
    "    def __init__(self, momentum=0.99, **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.mean = tf.Variable(0.0, trainable=False, validate_shape=False)\n",
    "        self.stddev = tf.Variable(0.0, trainable=False, validate_shape=False)\n",
    "        self._started = tf.Variable(False, trainable=False)\n",
    "        super(NormalisationOverall, self).__init__(**kwargs)\n",
    "        \n",
    "    def transform(self, X, y):\n",
    "        def first_time():\n",
    "            self._started.assign(True)\n",
    "            mean = tf.reduce_mean(X)\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(X - mean)))\n",
    "            self.mean.assign(mean)\n",
    "            self.stddev.assign(stddev)\n",
    "        \n",
    "        def later_times():\n",
    "            if self._training:\n",
    "                mean = tf.reduce_mean(X)\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(X - mean)))\n",
    "                self.mean.assign(self.mean * self.momentum + mean * (1 - self.momentum))\n",
    "                self.stddev.assign(self.stddev * self.momentum + stddev * (1 - self.momentum))\n",
    "        \n",
    "        tf.cond(tf.equal(self._started, False), first_time, later_times)\n",
    "        \n",
    "        X_scaled = (X - self.mean) / self.stddev\n",
    "        y_scaled = (y - self.mean) / self.stddev\n",
    "        return X_scaled, y_scaled\n",
    "    \n",
    "    def inverse_transform(self, y):\n",
    "        return (y * self.stddev) + self.mean\n",
    "    \n",
    "    \n",
    "### Processor Saving Hooks\n",
    "\n",
    "class ProcessorSavingHook(tft.SessionRunHook):\n",
    "    \"\"\"Saves the processor data.\"\"\"\n",
    "    \n",
    "    def __init__(self, processor, model_dir, save_secs=600, \n",
    "                 save_steps=None, **kwargs):\n",
    "        self.processor = processor\n",
    "        self.model_dir = model_dir\n",
    "        self._timer = tft.SecondOrStepTimer(every_secs=save_secs,\n",
    "                                            every_steps=save_steps)\n",
    "        self._global_step_tensor = None\n",
    "        super(ProcessorSavingHook, self).__init__(**kwargs)\n",
    "    \n",
    "    def begin(self):\n",
    "        self._global_step_tensor = tft.get_global_step()\n",
    "        \n",
    "    def after_create_session(self, session, coord):\n",
    "        global_step = session.run(self._global_step_tensor)\n",
    "        self._save(session, global_step)\n",
    "        self._timer.update_last_triggered_step(global_step)\n",
    "        \n",
    "    def before_run(self, run_context):\n",
    "        return tft.SessionRunArgs(self._global_step_tensor)\n",
    "        \n",
    "    def after_run(self, run_context, run_values):\n",
    "        stale_global_step = run_values.results\n",
    "        if self._timer.should_trigger_for_step(stale_global_step + 1):\n",
    "            global_step = run_context.session.run(self._global_step_tensor)\n",
    "            if self._timer.should_trigger_for_step(global_step):\n",
    "                self._timer.update_last_triggered_step(global_step)\n",
    "                self._save(run_context.session, global_step)\n",
    "            \n",
    "    def end(self, session):\n",
    "        last_step = session.run(self._global_step_tensor)\n",
    "        if last_step != self._timer.last_triggered_step():\n",
    "            self._save(session, last_step)\n",
    "        \n",
    "    def _save(self, session, step):\n",
    "        self.processor.save(self.model_dir, session, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DNN Construction\n",
    "\n",
    "# Keras-inspired nice interface, just without the slow speed and lack of \n",
    "# multicore functionality of Keras...\n",
    "\n",
    "class Sequential:\n",
    "    \"\"\"Defines a neural network. Expected usage is roughly:\n",
    "    \n",
    "    >>> model = Sequential()\n",
    "    >>> model.add(tf.layers.Dense(units=100, activation=tf.nn.relu))\n",
    "    >>> model.add_train(tf.layers.Dropout(rate=0.4))\n",
    "    >>> model.add(tf.layers.Dense(units=50, activation=tf.nn.relu))\n",
    "    >>> model.add_train(tf.layers.Dropout(rate=0.4))\n",
    "    >>> model.add(tf.layers.Dense(units=10, activation=tf.nn.relu))\n",
    "    \n",
    "    to define the neural network in the abstract (note that the last dense layer\n",
    "    are treated as the logits), followed by:\n",
    "    \n",
    "    >>> dnn = model.compile()\n",
    "    \n",
    "    to actually create it in TensorFlow. Here, 'dnn' is a tf.Estimator, so may\n",
    "    be used like:\n",
    "    \n",
    "    >>> dnn.train(...)\n",
    "    >>> dnn.predict(...)\n",
    "    >>> dnn.evaluate(...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Creates a Sequential. See Sequential.__doc__ for more info.\"\"\"\n",
    "        self._layer_funcs = []\n",
    "        self._layer_train = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        \"\"\"Add a layer to the network.\n",
    "        \"\"\"\n",
    "        self._layer_funcs.append(layer)\n",
    "        self._layer_train.append(False)\n",
    "        \n",
    "    def add_train(self, layer):\n",
    "        \"\"\"Add a layer to the network which needs to know if the network is in\n",
    "        training or not.\n",
    "        \"\"\"\n",
    "        self.add(layer)\n",
    "        self._layer_train[-1] = True\n",
    "        \n",
    "    def compile(self, optimizer=None, loss_fn=tflo.mean_squared_error, \n",
    "                model_dir=None, gradient_clip=None, processor=None, **kwargs):\n",
    "        \"\"\"Takes its abstract neural network definition and compiles it into a\n",
    "        tf.estimator.Estimator.\n",
    "        \n",
    "        May be given an :optimizer:, defaulting to tf.train.AdamOptimizer().\n",
    "        May be given a :loss_fn:, defaulting to tf.losses.mean_squared_error.\n",
    "        May be given a :gradient_clip:, defaulting to no clipping.\n",
    "        May be given a :processor:, which will be saved and loaded.\n",
    "        \n",
    "        Any additional kwargs are passed into the creation of the\n",
    "        tf.estimator.Estimator.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probably shouldn't use the same optimizer instance every time? Hence\n",
    "        # this.\n",
    "        if optimizer is None:\n",
    "            optimizer = tft.AdamOptimizer()\n",
    "            \n",
    "        if processor is None:\n",
    "            processor = IdentityProcessor()\n",
    "            \n",
    "        def model_fn(features, labels, mode):\n",
    "            def init_fn(scaffold, session):\n",
    "                if model_dir is not None:\n",
    "                    processor.load(session, model_dir)\n",
    "            scaffold = tft.Scaffold(init_fn=init_fn)\n",
    "            \n",
    "            # First layer is the feature inputs.\n",
    "            layers = [features[\"X\"]]\n",
    "            \n",
    "            for prev_layer, layer_func, train in zip(layers, self._layer_funcs, \n",
    "                                                     self._layer_train):\n",
    "                if train:\n",
    "                    layer = layer_func(inputs=prev_layer, \n",
    "                                       training=mode == tfe.ModeKeys.TRAIN)\n",
    "                else:\n",
    "                    layer = layer_func(inputs=prev_layer)\n",
    "                    \n",
    "                # Deliberately using the generator nature of zip to add elements\n",
    "                # to the layers list as we're iterating through it.\n",
    "                # https://media.giphy.com/media/3oz8xtBx06mcZWoNJm/giphy.gif\n",
    "                layers.append(layer)\n",
    "                \n",
    "            logits = layers[-1]\n",
    "            \n",
    "            if mode == tfe.ModeKeys.PREDICT:\n",
    "                return tfe.EstimatorSpec(mode=mode, predictions=logits, scaffold=scaffold)\n",
    "            \n",
    "            loss = loss_fn(labels, logits)\n",
    "\n",
    "            if mode == tfe.ModeKeys.TRAIN:\n",
    "                g_step = tft.get_global_step()\n",
    "                if gradient_clip is None:\n",
    "                    train_op = optimizer.minimize(loss=loss, global_step=g_step)\n",
    "                else:\n",
    "                    # Perform Gradient clipping\n",
    "                    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "                    with tf.control_dependencies(update_ops):\n",
    "                        gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "#                         gradients0 = tf.Print(gradients[0], [tf.global_norm(gradients)], 'Global norm: ')\n",
    "#                         gradients = tuple([gradients0, *gradients[1:]])\n",
    "                        gradients, _ = tf.clip_by_global_norm(gradients, \n",
    "                                                              gradient_clip)\n",
    "                        train_op = optimizer.apply_gradients(zip(gradients, \n",
    "                                                                 variables),\n",
    "                                                             global_step=g_step)\n",
    "                training_hooks = [] if model_dir is None else [ProcessorSavingHook(processor, model_dir)]\n",
    "                return tfe.EstimatorSpec(mode=mode, loss=loss, train_op=train_op,\n",
    "                                         scaffold=scaffold,\n",
    "                                         training_hooks=training_hooks)\n",
    "            \n",
    "            if mode == tfe.ModeKeys.EVAL:\n",
    "                return tfe.EstimatorSpec(mode=mode, loss=loss, scaffold=scaffold)\n",
    "            \n",
    "            raise RuntimeError(\"mode '{}' not understood\".format(mode))\n",
    "                \n",
    "        return tfe.Estimator(model_fn=model_fn, model_dir=model_dir, **kwargs)\n",
    "\n",
    "    \n",
    "def create_dnn(hidden_units, logits, activation=tf.nn.relu, \n",
    "               drop_rate=0.0, drop_type='dropout', model_dir=None, log_steps=100, \n",
    "               gradient_clip=None, batch_norm=False, processor=None,\n",
    "               kernel_initializer=tfi.truncated_normal(mean=0, stddev=0.05),\n",
    "               **kwargs):\n",
    "    \"\"\"Shortcut for creating a simple DNN with dense, dropout and batch \n",
    "    normalization layers, and then compiling it.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _create_dnn():\n",
    "        model = Sequential()\n",
    "        if batch_norm:\n",
    "            model.add_train(tfla.BatchNormalization())\n",
    "        for units in hidden_units:\n",
    "            model.add(tfla.Dense(units=units, activation=activation,\n",
    "                                 kernel_initializer=kernel_initializer))\n",
    "            if batch_norm:\n",
    "                model.add_train(tfla.BatchNormalization())\n",
    "            if drop_rate != 0:\n",
    "                if drop_type in ('normal', 'dropout'):\n",
    "                    model.add_train(tfla.Dropout(rate=drop_rate))\n",
    "                elif drop_type in ('alpha', 'alpha_dropout'):\n",
    "                    model.add_train(tfk.layers.AlphaDropout(rate=drop_rate))\n",
    "        model.add(tf.layers.Dense(units=logits, \n",
    "                                  kernel_initializer=kernel_initializer))\n",
    "        \n",
    "        return model.compile(gradient_clip=gradient_clip, \n",
    "                             processor=processor,\n",
    "                             model_dir=model_dir,\n",
    "                             config=tfe.RunConfig(log_step_count_steps=log_steps),\n",
    "                             **kwargs)\n",
    "\n",
    "    return _create_dnn\n",
    "    \n",
    "    \n",
    "def model_dir_str(model_dir, hidden_units, logits, processor, activation, \n",
    "                  uuid=None):\n",
    "    \"\"\"Returns a string for the model directory describing the network.\"\"\"\n",
    "    \n",
    "    layer_counter = [(k, sum(1 for _ in g)) for k, g in it.groupby(hidden_units)]\n",
    "    for layer_size, layer_repeat in layer_counter:\n",
    "        if layer_repeat == 1:\n",
    "            model_dir += '{}_'.format(layer_size)\n",
    "        else:\n",
    "            model_dir += '{}x{}_'.format(layer_size, layer_repeat)\n",
    "    model_dir += '{}__'.format(logits)\n",
    "    model_dir += processor.__class__.__name__\n",
    "    \n",
    "    if isinstance(activation, ft.partial):\n",
    "        activation_fn = activation.func\n",
    "        alpha = str(activation.keywords['alpha']).replace('.', '')\n",
    "    else:\n",
    "        activation_fn = activation\n",
    "        alpha = '02'\n",
    "        \n",
    "    model_dir += '_' + activation_fn.__name__.replace('_', '')\n",
    "    if activation_fn is tf.nn.leaky_relu:\n",
    "        model_dir += alpha\n",
    "\n",
    "    if uuid not in (None, ''):\n",
    "        model_dir += '_' + str(uuid)\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "def dnn_hyperparameters_from_dir(dir_name):\n",
    "    \"\"\"Creates DNN hyperparameters from the name of the directory of the DNN.\n",
    "    \"\"\"\n",
    "    \n",
    "    dnn_details = {}\n",
    "\n",
    "    units, rest = dir_name.split('__')\n",
    "    units = units.split('_')\n",
    "    rest = rest.split('_')\n",
    "    \n",
    "    all_units = []\n",
    "    for unit in units:\n",
    "        if 'x' in unit:\n",
    "            unit_size, unit_repeat = unit.split('x')\n",
    "            unit_size, unit_repeat = int(unit_size), int(unit_repeat)\n",
    "            all_units.extend([unit_size for _ in range(unit_repeat)])\n",
    "        else:\n",
    "            all_units.append(int(unit))\n",
    "    dnn_details['hidden_units'] = all_units[:-1]\n",
    "    dnn_details['logits'] = all_units[-1]\n",
    "    \n",
    "    processor_name = rest[0]\n",
    "    processor_class = Processor.find_subclass(processor_name)\n",
    "    dnn_details['processor'] = processor_class()\n",
    "    dnn_details['batch_norm'] = False\n",
    "    \n",
    "    activation_name = rest[1].lower()\n",
    "    \n",
    "    # Not a great way to do this inversion, admittedly\n",
    "    if activation_name[:9] == 'leakyrelu':\n",
    "        alpha = float(str(activation_name[9]) + '.' + str(activation_name[10:]))\n",
    "        dnn_details['activation'] = ft.partial(tf.nn.leaky_relu, alpha=alpha)\n",
    "    else:\n",
    "        try:\n",
    "            activation_fn = getattr(tf.nn, activation_name)\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"Activation '{}' not understood.\".format(activation_name))\n",
    "        else:\n",
    "            dnn_details['activation'] = activation_fn\n",
    "        \n",
    "    remaining = rest[2:]\n",
    "    if len(remaining) == 0:\n",
    "        uuid = None\n",
    "    elif len(remaining) == 1:\n",
    "        uuid = remaining[0]\n",
    "    else:\n",
    "        raise RuntimeError(\"Bad dir_name string '{}'. Too many remaining \"\n",
    "                           \"arguments: {}\".format(dir_name, remaining))\n",
    "        \n",
    "    return dnn_details, uuid\n",
    "\n",
    "\n",
    "def create_dnn_from_model_dir(model_dir, **kwargs):\n",
    "    \"\"\"Creates a DNN and processor from their model_dir.\"\"\"\n",
    "    if model_dir[-1] in ('/', '\\\\'):\n",
    "        model_dir = model_dir[:-1]\n",
    "    model_dir_split = tools.split(['/', '\\\\'], model_dir)\n",
    "    dir_name = model_dir_split[-1]\n",
    "    dnn_details, uuid = dnn_hyperparameters_from_dir(dir_name)\n",
    "    dnn_creator = create_dnn(model_dir=model_dir, **dnn_details, **kwargs)\n",
    "    dnn = dnn_creator()\n",
    "    return dnn, uuid\n",
    "\n",
    "\n",
    "def create_dnns_from_dir(dir_, log_steps=100, gradient_clip=None, \n",
    "                         exclude_start=('.',), exclude_end=(),\n",
    "                         exclude_in=()):\n",
    "    \"\"\"Creates multiple DNNs and processors from a directory containing the\n",
    "    directories for multiple DNNs and processors.\n",
    "    \"\"\"\n",
    "    \n",
    "    subdirectories = sorted(next(os.walk(dir_))[1])\n",
    "    if dir_[-1] in ('/', '\\\\'):\n",
    "        dir_ = dir_[:-1]\n",
    "    dnns_names = []\n",
    "    \n",
    "    for subdir in subdirectories:\n",
    "        if any(subdir.startswith(ex) for ex in exclude_start):\n",
    "            tflog.warn(\"Excluding '{}' based on start.\".format(subdir))\n",
    "            continue\n",
    "        if any(subdir.endswith(ex) for ex in exclude_end):\n",
    "            tflog.warn(\"Excluding '{}' based on end.\".format(subdir))\n",
    "            continue\n",
    "        if any(ex in subdir for ex in exclude_in):\n",
    "            tflog.warn(\"Excluding '{}' based on containment.\".format(subdir))\n",
    "            continue\n",
    "            \n",
    "        model_dir = dir_ + '/' + subdir\n",
    "        try:\n",
    "            dnn, uuid = create_dnn_from_model_dir(model_dir, log_steps=log_steps,\n",
    "                                                  gradient_clip=gradient_clip)\n",
    "        except (FileNotFoundError, RuntimeError) as e:\n",
    "            tflog.warn(\"Could not load DNN from '{}'. Error message: '{}'\"\n",
    "                       .format(subdir, e))\n",
    "        else:\n",
    "            dnns_names.append((dnn, subdir))\n",
    "            \n",
    "    return dnns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simpler interpolation methods\n",
    "# Useful to give a baseline to compare the neural network models against.\n",
    "\n",
    "class _RegressorBase:\n",
    "    \"\"\"Base class for performing predictions based on just the input. Subclasses\n",
    "    are expected to provide a predict_single classmethod specifying their\n",
    "    predictions.\n",
    "    \n",
    "    Its predict and evaluate methods are designed to resemble that of\n",
    "    tf.estimator.Estimator's, so that we can call them in the same way. (We don't\n",
    "    actually inherit from tf.estimator.Estimator because none of what these \n",
    "    classes do uses TensorFlow, so messing around with model functions and\n",
    "    EstimatorSpecs is just unnecessary faff and overhead.)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flag to tell test_regressor (and thus BatchData) that this regressor\n",
    "    # doesn't use TensorFlow.\n",
    "    use_tf = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _index_tol(cg, point, tol=0.001):\n",
    "        \"\"\"Searches through a list of 2-tuples, :cg:, to find the first element \n",
    "        which is within tolerance :tol: of :point:. Essentially the index method\n",
    "        for lists, except this one makes sense for high precision floating point\n",
    "        numbers.\n",
    "        \"\"\"\n",
    "        \n",
    "        t, x = point\n",
    "        for i, element in enumerate(cg):\n",
    "            t2, x2 = element\n",
    "            if max(np.abs(t - t2), np.abs(x - x2)) < tol:\n",
    "                return i\n",
    "        raise ValueError('{} is not in {}'.format(point, type(cg)))\n",
    "        \n",
    "    def _prepare(self, Xi):\n",
    "        \"\"\"Performs any necessary preparations on the data :Xi: before making \n",
    "        predictions.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _interp(self, Xi, point):\n",
    "        \"\"\"Helper function for performing interpolation on a coarse\n",
    "        grid :Xi:, giving the value of the interpolation at :point:.\n",
    "        \n",
    "        The spacing of the grid is known from the global hyperparameters\n",
    "        defining the coarse grid size, whilst it isn't necessary to know its\n",
    "        location.\n",
    "        \n",
    "        The argument :point: should be scaled to the grid size, i.e.\n",
    "        coarse_grid_sep.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict_single(self, Xi, y):\n",
    "        \"\"\"Makes a prediction corresponding to input feature :Xi:.\n",
    "        \n",
    "        It is given the true result :y:. Not to cheat and return perfect\n",
    "        results, but to determine its shape etc.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict(self, input_fn, yield_single_examples=False):\n",
    "        \"\"\"The argument :input_fn: should probably be an instance of BatchData.\n",
    "        \n",
    "        The argument :yield_single_examples: is there for compatibility with the\n",
    "        interface for the usual TF Estimators and is ignored.\n",
    "        \"\"\"\n",
    "        \n",
    "        returnval = []\n",
    "        X, y = input_fn()\n",
    "        \n",
    "        for Xi in X['X']:\n",
    "            returnval.append(self.predict_single(Xi, y))\n",
    "            \n",
    "        returnval = np.array(returnval)\n",
    "        while True:\n",
    "            yield returnval\n",
    "            \n",
    "    def evaluate(self, input_fn, steps=None):\n",
    "        \"\"\"The argument :input_fn: should probably be an instance of BatchData.\n",
    "        \n",
    "        The number of steps is given by :steps:, with None corresponding to\n",
    "        infinity. The evaluation will terminate earlier if input_fn raises a\n",
    "        StopIteration or a tf.errors.OutOfRangeError.\"\"\"\n",
    "        \n",
    "        if steps is None:\n",
    "            steps = np.inf\n",
    "        \n",
    "        losses = []\n",
    "        for step in tools.rangeinf(0, steps):\n",
    "            try:\n",
    "                X, y = input_fn()\n",
    "            except (StopIteration, tfer.OutOfRangeError):\n",
    "                break\n",
    "            predictor = self.predict(lambda: (X, y))\n",
    "            prediction = next(predictor)\n",
    "            losses.append(np.mean(np.square(y - prediction)))\n",
    "            \n",
    "        return {'loss': np.mean(losses), 'global_step': None}\n",
    "\n",
    "\n",
    "class BilinearInterpBase(_RegressorBase):\n",
    "    \"\"\"Base class for performing bilinear interpolation.\"\"\"\n",
    "        \n",
    "    def _interp(self, Xi, point):        \n",
    "        # The actual t, x values for the grid don't matter from this point \n",
    "        # onwards; so this is just a translation from wherever X was actually \n",
    "        # calculated. So WLOG assume it was around 0.\n",
    "        cg = coarse_grid((0, 0))\n",
    "        t, x = point\n",
    "        \n",
    "        # The grid points nearest :point:.\n",
    "        t_below = tools.round_mult(t, coarse_grid_sep.t, 'down')\n",
    "        t_above = tools.round_mult(t, coarse_grid_sep.t, 'up')\n",
    "        x_below = tools.round_mult(x, coarse_grid_sep.x, 'down')\n",
    "        x_above = tools.round_mult(x, coarse_grid_sep.x, 'up')\n",
    "        \n",
    "        # The value of :Xi: at those grid points.\n",
    "        t_b_x_b = Xi[self._index_tol(cg, (t_below, x_below))]\n",
    "        t_a_x_b = Xi[self._index_tol(cg, (t_above, x_below))]\n",
    "        t_b_x_a = Xi[self._index_tol(cg, (t_below, x_above))]\n",
    "        t_a_x_a = Xi[self._index_tol(cg, (t_above, x_above))]\n",
    "        \n",
    "        # Shift the t, x values to be relative to the bottom-left point of the\n",
    "        # grid square in which (t, x) lies.\n",
    "        t_scale = (t % coarse_grid_sep.t) / coarse_grid_sep.t\n",
    "        x_scale = (x % coarse_grid_sep.x) / coarse_grid_sep.x\n",
    "        \n",
    "        # Bilinear interpolation\n",
    "        returnval = (1 - t_scale) * (1 - x_scale) * t_b_x_b\n",
    "        returnval += t_scale * (1 - x_scale) * t_a_x_b\n",
    "        returnval += (1 - t_scale) * x_scale * t_b_x_a\n",
    "        returnval += t_scale * x_scale * t_a_x_a\n",
    "        \n",
    "        return returnval\n",
    "    \n",
    "    \n",
    "class PolyInterpBase(_RegressorBase):\n",
    "    \"\"\"Base class for performing polynomial interpolation.\"\"\"\n",
    "    \n",
    "    def __init__(self, poly_deg, *args, **kwargs):\n",
    "        self.poly_deg = poly_deg\n",
    "        self._poly_coefs = None\n",
    "        super(PolyInterpBase, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def poly(self, point):\n",
    "        \"\"\"Interprets its currently stored polynomial coefficients as a \n",
    "        polynomial, and evaluates them at the specified point.\"\"\"\n",
    "        \n",
    "        if self._poly_coefs is None:\n",
    "            raise RuntimeError('Must run _prepare first!')\n",
    "        \n",
    "        t, x = point\n",
    "        coefs = iter(self._poly_coefs)\n",
    "\n",
    "        result = next(coefs)  # Intercept, i.e. constant term\n",
    "        for power in range(1, self.poly_deg + 1):\n",
    "            for x_power in range(0, power + 1):\n",
    "                t_power = power - x_power\n",
    "                coef = next(coefs)\n",
    "                result += coef * (t ** t_power) * (x ** x_power)\n",
    "        try:\n",
    "            next_coef = next(coefs)\n",
    "        except StopIteration:\n",
    "            return result\n",
    "        else:\n",
    "            raise RuntimeError('coef_: {coef_}, poly_deg: {poly_deg}, '\n",
    "                               'coef that shouldn\\'t exist: {next_coef}'\n",
    "                               .format(coef_=coef_, \n",
    "                                       poly_deg=self.poly_deg, \n",
    "                                       next_coef=next_coef))\n",
    "    \n",
    "    def _prepare(self, Xi):\n",
    "        poly_features = skpr.PolynomialFeatures(degree=self.poly_deg, \n",
    "                                                include_bias=True)\n",
    "        lin_reg = sklm.LinearRegression(fit_intercept=False)\n",
    "        poly_pipe = skpi.Pipeline([('pf', poly_features), ('lr', lin_reg)])\n",
    "        \n",
    "        # The actual t, x values for the grid don't matter from this point \n",
    "        # onwards; so this is just a translation from wherever X was actually \n",
    "        # calculated. So WLOG assume it was around 0.\n",
    "        cg = coarse_grid((0, 0))\n",
    "        poly_pipe.fit(cg, Xi)\n",
    "        self._poly_coefs = poly_pipe.named_steps['lr'].coef_\n",
    "        \n",
    "    \n",
    "    def _interp(self, Xi, point):\n",
    "        return self.poly(point)\n",
    "    \n",
    "    \n",
    "class NearestInterpBase(_RegressorBase):\n",
    "    \"\"\"Base class for performing nearest-neighbour interpolation.\"\"\"\n",
    "        \n",
    "    def _interp(self, Xi, point):        \n",
    "        # The actual t, x values for the grid don't matter from this point \n",
    "        # onwards; so this is just a translation from wherever X was actually \n",
    "        # calculated. So WLOG assume it was around 0.\n",
    "        cg = coarse_grid((0, 0))\n",
    "        t, x = point\n",
    "        \n",
    "        # The grid point nearest :point:.\n",
    "        t_nearest = tools.round_mult(t, coarse_grid_sep.t, 'round')\n",
    "        x_nearest = tools.round_mult(x, coarse_grid_sep.x, 'round')\n",
    "        \n",
    "        # The value of :Xi: at those grid points.\n",
    "        t_n_x_n = Xi[self._index_tol(cg, (t_nearest, x_nearest))]\n",
    "\n",
    "        return t_n_x_n\n",
    "    \n",
    "    \n",
    "class FineGridPredictorMixin(_RegressorBase):\n",
    "    \"\"\"Provides the predict_single function for predictions on a fine grid.\"\"\"\n",
    "    \n",
    "    def predict_single(self, Xi, y):\n",
    "        returnval = []\n",
    "        # Translation doesn't matter at this point so WLOG the fine grid is\n",
    "        # around 0, 0. (cls._interp makes the same assumption; these assumptions\n",
    "        # must be consistent)\n",
    "        for point in fine_grid((0, 0)):\n",
    "            self._prepare(Xi)\n",
    "            returnval.append(self._interp(Xi, point))\n",
    "        return returnval\n",
    "    \n",
    "    \n",
    "class PointPredictorMixin(_RegressorBase):\n",
    "    \"\"\"Provides the predict_single function for predictions at a single point.\"\"\"\n",
    "    \n",
    "    def predict_single(self, Xi):\n",
    "        # Separate the location data and the grid data\n",
    "        t_offset = Xi[-2]\n",
    "        x_offset = Xi[-1]\n",
    "        Xi = Xi[:-2]\n",
    "        self._prepare(Xi)\n",
    "        # Wrapped in a list for consistency: this network just happens to only\n",
    "        # be trying to predict a single label.\n",
    "        return [self._interp(Xi, (t_offset * coarse_grid_sep.t, \n",
    "                                  x_offset * coarse_grid_sep.x))]\n",
    "    \n",
    "    \n",
    "class Perfect(_RegressorBase):\n",
    "    \"\"\"Regressor that cheats to always give the perfect prediction.\"\"\"\n",
    "    \n",
    "    def predict_single(self, Xi, y):\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using regressors\n",
    "\n",
    "def _use_regressor(regressor, processor):\n",
    "    \"\"\"Has a regressor make a prediction on some test data.\n",
    "    \n",
    "    The :regressor: argument should probably be an instance of\n",
    "    tf.estimator.Estimator or of _RegressorBase.\n",
    "    \n",
    "    The :processor: argument should be an processor with data, i.e. it\n",
    "    has already had its data function called on some BatchData.\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(regressor, 'use_tf'):\n",
    "        use_tf = regressor.use_tf\n",
    "    else:\n",
    "        use_tf = True\n",
    "    \n",
    "    processor.training(False)\n",
    "    predictor = regressor.predict(input_fn=processor(use_tf),\n",
    "                                  yield_single_examples=False)\n",
    "    prediction = next(predictor)\n",
    "    prediction_with_postprocessing = processor.inverse_transform(prediction)\n",
    "    raw_X = processor.batch_data.X\n",
    "    raw_y = processor.batch_data.y\n",
    "    diff = prediction_with_postprocessing - raw_y\n",
    "    squared_error = np.square(diff)\n",
    "    result = tools.Object(prediction=prediction_with_postprocessing,\n",
    "                          X=raw_X,\n",
    "                          y=raw_y,\n",
    "                          diff=diff,\n",
    "                          average_loss=np.mean(squared_error),\n",
    "                          loss=np.sum(squared_error))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using regressors in ensembles\n",
    "\n",
    "class RegressorAverager:\n",
    "    \"\"\"Regressors that averages the results of other regressors to make its\n",
    "    prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    use_tf = False\n",
    "    \n",
    "    def __init__(self, regressors_processors=None, \n",
    "                 regressors_processors_names=None, \n",
    "                 mask=None, \n",
    "                 **kwargs):\n",
    "        \"\"\"Should be passed an iterable of regressors and their processors.\n",
    "        This can either be via a tuple of 2-tuples :regressors_processors:,\n",
    "        or for convenience with the output of the create_dnns_pros_from_dir\n",
    "        function, may be a tuple of 3-tuples :regressors_processors_names:.\n",
    "        (For which the name part is ignored.) Precisely one of these \n",
    "        arguments must be passed.\n",
    "        \n",
    "        May also pass a :mask: argument, which should be a tuple of bools\n",
    "        the same length as the number of (regressor, processor) pairs,\n",
    "        specifying whether or not that regressor should be used when making\n",
    "        predictions.\n",
    "        \"\"\"\n",
    "        if (regressors_processors is None and \\\n",
    "                                    regressors_processors_names is None) or \\\n",
    "           (regressors_processors is not None and \\\n",
    "                                regressors_processors_names is not None):\n",
    "            raise RuntimeError(\"Precisely only of 'regressors_processors' and \"\n",
    "                               \"'regressors_processors_names' must be not None.\")\n",
    "            \n",
    "        elif regressors_processors_names is not None:\n",
    "            regressors_processors = tuple((reg, pro) for reg, pro, name \n",
    "                                          in regressors_processors_names)\n",
    "            \n",
    "        self.regressors_processors = tuple(regressors_processors)\n",
    "        self.mask = None\n",
    "        self.reset_mask()\n",
    "        if mask is not None:\n",
    "            self.set_mask(mask)\n",
    "        super(RegressorAverager, self).__init__(**kwargs)\n",
    "        \n",
    "    def set_mask(self, mask):\n",
    "        assert len(mask) == len(self.regressors_with_pro)\n",
    "        self.mask = tuple(mask)\n",
    "        return self  # for chaining\n",
    "        \n",
    "    def reset_mask(self):\n",
    "        self.mask = [True for _ in range(len(self.regressors_with_pro))]\n",
    "        return self  # for chaining\n",
    "        \n",
    "    def predict(self, input_fn, *args, **kwargs):\n",
    "        \"\"\"The argument :input_fn: should probably be an instance of BatchData.\n",
    "        \"\"\"\n",
    "        \n",
    "        X, y = input_fn()\n",
    "        X = X['X']\n",
    "        test_data = BatchData.from_single_data(X, y)\n",
    "        \n",
    "        returnval = tools.AddBase()\n",
    "        counter = 0\n",
    "        for (regressor, processor), mask in zip(self.regressors_with_pro, self.mask):\n",
    "            if mask:\n",
    "                counter += 1\n",
    "                if processor is None:\n",
    "                    processor = IdentityProcessor()\n",
    "                processor.data(test_data)\n",
    "                returnval += _use_regressor(regressor, processor).prediction\n",
    "        returnval = returnval / counter\n",
    "        \n",
    "        while True:\n",
    "            yield returnval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing and evaluating regressors\n",
    "\n",
    "def test_regressors(regressors_processors, gen_raw_data, batch_size=1):\n",
    "    \"\"\"Tests a list of regressors on some test data. The regressors\n",
    "    may optionally have some preprocessing applied to their inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    test_data = BatchData.test(gen_raw_data, batch_size)\n",
    "    return test_regressors_on_batch_data(regressors_processors, test_data)\n",
    "\n",
    "\n",
    "def test_regressors_on_batch_data(regressors_processors, test_data):\n",
    "    \"\"\"As test_regressors, but takes a BatchData instance as an input \n",
    "    instead.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for regressor, processor in regressors_processors:\n",
    "        if processor is None:\n",
    "            processor = IdentityProcessor()\n",
    "        processor.data(test_data)\n",
    "        result = _use_regressor(regressor, processor)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualising the results of regressors\n",
    "\n",
    "# Only plots fine grid style stuff at the moment\n",
    "def plot_regressors(regressors_processors_names, X, y):\n",
    "    \"\"\"Plots the results of some regressors using the given data.\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8 * len(regressors_with_info)))\n",
    "    \n",
    "    test_data = BatchData.from_single_data(X, y)\n",
    "    \n",
    "    regressors_processors = [xx[:2] for xx in regressors_processors_names]\n",
    "    names = [xx[2] for xx in regressors_processors_names]\n",
    "    \n",
    "    results = test_regressors_on_batch_data(regressors_processors, test_data)\n",
    "    \n",
    "    for i, (result, name) in enumerate(zip(results, names)):\n",
    "        ax = make_3d_ax_for_grid_plotting(fig, (len(regressors_processors_names), 1, i + 1))\n",
    "        grid_plot(ax, X, 'cg', '_nolegend_')\n",
    "        grid_plot(ax, result.prediction, 'fg', name)\n",
    "        ax.legend()\n",
    "        \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
