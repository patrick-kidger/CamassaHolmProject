{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies:\n",
    "#  base_eq.ipynb\n",
    "#  base_plot.ipynb\n",
    "#\n",
    "# These should be %run before this file.\n",
    "# (Can't %run them here as this file is not necessarily in the\n",
    "# same directory as the file %run-ing this file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we'd split this file up into a few other files, but then\n",
    "# we'd have even more %run dependencies when it comes to actually\n",
    "# using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "import sklearn.base as skb\n",
    "import sklearn.preprocessing as skpr\n",
    "import sklearn.pipeline as skpi\n",
    "import sklearn.linear_model as sklm\n",
    "\n",
    "# https://github.com/patrick-kidger/tools\n",
    "import tools\n",
    "\n",
    "import tensorflow as tf\n",
    "tfd = tf.data\n",
    "tfer = tf.errors\n",
    "tfe = tf.estimator\n",
    "tfi = tf.initializers\n",
    "tfk = tf.keras\n",
    "tfla = tf.layers\n",
    "tflog = tf.logging\n",
    "tflo = tf.losses\n",
    "tft = tf.train\n",
    "\n",
    "# Convenience imports for those files running this one\n",
    "import collections as co\n",
    "import functools as ft\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid hyperparameters\n",
    "# Everything we do is on a grid\n",
    "\n",
    "# The separation between points of the fine grid\n",
    "fine_grid_sep = tools.Object(t=0.01, x=0.01)\n",
    "# The separation between points of the coarse grid\n",
    "coarse_grid_sep = tools.Object(t=0.1, x=0.1)\n",
    "# The amount of intervals in the coarse grid. Thus the coarse grid will contain\n",
    "# (num_intervals.t + 1) * (num_intervals.x + 1) elements.\n",
    "# So with num_intervals.t = 3, num_intervals.x = 3, it looks like:\n",
    "#\n",
    "# @ @ @ @\n",
    "#\n",
    "# @ @ @ @\n",
    "#\n",
    "# @ @ @ @\n",
    "#\n",
    "# @ @ @ @\n",
    "num_intervals = tools.Object(t=7, x=7)\n",
    "\n",
    "\n",
    "fine_grid_fineness = tools.Object(t=int(coarse_grid_sep.t // fine_grid_sep.t), \n",
    "                                  x=int(coarse_grid_sep.x // fine_grid_sep.x))\n",
    "coarse_grid_size = tools.Object(t=num_intervals.t * coarse_grid_sep.t,\n",
    "                                x=num_intervals.x * coarse_grid_sep.x)\n",
    "\n",
    "\n",
    "### Grids to evaluate our solution on\n",
    "\n",
    "def grid(point, grid_size, grid_fineness):\n",
    "    \"\"\"Creates a grid whose bottom left entry is at the specified :point:\n",
    "    location. The size of the overall grid may be specified via :grid_size:, and\n",
    "    the fineness of the subdivision by :grid_fineness:, both of which should be\n",
    "    of the form tools.Object(t, x). Thus the resulting grid has\n",
    "    (grid_fineness.t + 1) * (grid_fineness.x + 1) elements.\"\"\"\n",
    "    t, x = point\n",
    "    return [(t_, x_) for t_ in np.linspace(t, t + grid_size.t, \n",
    "                                           grid_fineness.t + 1)\n",
    "                     for x_ in np.linspace(x, x + grid_size.x, \n",
    "                                           grid_fineness.x + 1)]\n",
    "\n",
    "def fine_grid(point):\n",
    "    \"\"\"Creates a fine grid whose bottom left entry is at the specified :point:\n",
    "    location, with size and fineness determined by the earlier hyperparameters.\n",
    "    \"\"\"\n",
    "    return grid(point, coarse_grid_sep, fine_grid_fineness)\n",
    "\n",
    "def coarse_grid(point):\n",
    "    \"\"\"Creates a coarse grid for which the bottom left entry of its middle\n",
    "    square is as the specified :t:, :x: location, with size and fineness\n",
    "    determined by the earlier hyperparameters.\n",
    "    \"\"\"\n",
    "    left_intervals_t = np.floor((num_intervals.t - 1) / 2)\n",
    "    left_intervals_x = np.floor((num_intervals.x - 1) / 2)\n",
    "    \n",
    "    left_amount_t = left_intervals_t * coarse_grid_sep.t\n",
    "    left_amount_x = left_intervals_x * coarse_grid_sep.x\n",
    "    \n",
    "    t, x = point\n",
    "    bottomleft_point = (t - left_amount_t, x - left_amount_x)\n",
    "    return grid(bottomleft_point, coarse_grid_size, num_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data generation\n",
    "\n",
    "# These next four functions may be combined to generate (feature, label)\n",
    "# pairs, e.g.\n",
    "# >>> point, solution = gen_one_peakon()\n",
    "# >>> X, y = sol_on_grid(point, solution)\n",
    "def gen_one_peakon():\n",
    "    \"\"\"Returns a random peakon and a random location.\"\"\"\n",
    "    # Random solution to the CH equation\n",
    "    c = np.random.uniform(3, 10)\n",
    "    peakon = Peakon(c=c)\n",
    "    # Random location near the peak\n",
    "    t = np.random.uniform(0, 10)\n",
    "    x = c * t + np.random.uniform(-2, 2)\n",
    "    return (t, x), peakon\n",
    "\n",
    "def gen_two_peakon():\n",
    "    \"\"\"Returns a random two peakon solution, and a random location.\"\"\"\n",
    "    # Random solution to the CH equation\n",
    "    p1 = np.random.uniform(3, 10)\n",
    "    p2 = np.random.uniform(3, 10)\n",
    "    x1 = np.random.uniform(0, 3)\n",
    "    x2 = np.random.uniform(3.001, 6)\n",
    "    twopeakon = TwoPeakon(x1, x2, p1, p2)\n",
    "    # Random location near both of the peaks\n",
    "    t = np.random.uniform(0, 0.5)\n",
    "    left = min(x1 - 0.5 + p1 * t, x2 - 0.5 + p2 * t)\n",
    "    right = max(x1 + 0.5 + p1 * t, x2 + 0.5 + p2 * t)\n",
    "    middle = (right + left) / 2\n",
    "    semidist = (right - left) / 2\n",
    "    x = middle + semidist * np.random.uniform(-1, 1) ** 3\n",
    "    return (t, x), twopeakon\n",
    "\n",
    "def sol_on_grid(point, solution):\n",
    "    \"\"\"Returns the values of the :solution: on fine and coarse grids around the\n",
    "    specified :point:.\n",
    "    \"\"\"\n",
    "    # Grids at the location\n",
    "    cg = coarse_grid(point)\n",
    "    fg = fine_grid(point)\n",
    "    # Features: the solution on the coarse grid\n",
    "    X = solution.on_grid(cg)\n",
    "    # Labels: the solution on the fine grid\n",
    "    y = solution.on_grid(fg)\n",
    "    return X, y\n",
    "\n",
    "def sol_at_point(point, solution):\n",
    "    \"\"\"Returns the values of the :solution: on a coarse grid and at a random\n",
    "    point near the specified :point:.\n",
    "    \"\"\"\n",
    "    \n",
    "    cg = coarse_grid(point)\n",
    "    \n",
    "    # Random offset from the random location that we ask for predictions at. The\n",
    "    # distribution is asymmetric because we're moving relative to :point:, which\n",
    "    # is in the _bottom left_ of the central cell of the coarse grid. The asymmetric\n",
    "    # distribution thus makes this relative to te centre of the central cell.\n",
    "    #\n",
    "    # This value is not scaled relative to the size of the grid as we expect\n",
    "    # that the predictions should be scale invariant, and we do not want the\n",
    "    # network to unnecessarily learn the size of coarse_grid_sep.\n",
    "    x_offset = np.random.uniform(-0.5, 1.5)\n",
    "    t_offset = np.random.uniform(-0.5, 1.5)\n",
    "    \n",
    "    # Features: the solution on the coarse grid and the point to interpolate at.\n",
    "    X = solution.on_grid(cg, extra=2)\n",
    "    # We tell the network the offset; as the network has no way of knowing the\n",
    "    # location of the grid then adding a translation would only confuse it.\n",
    "    X[-2] = t_offset - 0.5  # -0.5 to normalise\n",
    "    X[-1] = x_offset - 0.5  # -0.5 to normalise\n",
    "    \n",
    "    t, x = point\n",
    "    # Label: the solution at the interpolation point\n",
    "    y = np.full(1, peakon((t + t_offset * coarse_grid_sep.t, \n",
    "                           x + x_offset * coarse_grid_sep.x)))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# A particularly nice X, y that is right on the peak of the peakon\n",
    "X_peak = np.array([0.71136994, 0.64367414, 0.58242045, 0.52699581, 0.47684553,\n",
    "                   0.43146768, 0.3904081 , 0.35325586, 1.53965685, 1.39313912,\n",
    "                   1.26056441, 1.14060584, 1.03206285, 0.93384908, 0.84498159,\n",
    "                   0.76457096, 3.33236346, 3.01524715, 2.72830845, 2.46867557,\n",
    "                   2.23375003, 2.02118061, 1.82883985, 1.65480272, 7.21241639,\n",
    "                   6.52606422, 5.9050271 , 5.34308947, 4.83462728, 4.37455167,\n",
    "                   3.95825804, 3.58157998, 3.81911647, 4.22077645, 4.66467938,\n",
    "                   5.155268  , 5.69745227, 6.29665855, 6.95888391, 7.69075612,\n",
    "                   1.76455206, 1.95013162, 2.15522875, 2.38189614, 2.63240234,\n",
    "                   2.90925451, 3.21522348, 3.55337148, 0.81527861, 0.90102221,\n",
    "                   0.99578354, 1.10051101, 1.21625277, 1.34416719, 1.48553448,\n",
    "                   1.64176951, 0.37668439, 0.41630063, 0.46008335, 0.50847074,\n",
    "                   0.56194707, 0.62104756, 0.68636371, 0.75854921])\n",
    "y_peak = np.array([5.34308947, 5.28992485, 5.23728921, 5.18517732, 5.13358394,\n",
    "                   5.08250393, 5.03193217, 4.98186361, 4.93229323, 4.8832161 ,\n",
    "                   4.83462728, 5.77198627, 5.71455405, 5.65769329, 5.6013983 ,\n",
    "                   5.54566345, 5.49048318, 5.43585196, 5.38176433, 5.32821488,\n",
    "                   5.27519826, 5.22270916, 6.23531118, 6.1732688 , 6.11184375,\n",
    "                   6.05102988, 5.99082113, 5.93121147, 5.87219493, 5.81376561,\n",
    "                   5.75591767, 5.69864534, 5.64194287, 6.73582778, 6.66880518,\n",
    "                   6.60244946, 6.53675399, 6.4717122 , 6.40731759, 6.34356371,\n",
    "                   6.2804442 , 6.21795273, 6.15608307, 6.09482902, 7.27652151,\n",
    "                   7.20411891, 7.13243673, 7.0614678 , 6.99120502, 6.92164137,\n",
    "                   6.85276989, 6.78458369, 6.71707595, 6.65023993, 6.58406894,\n",
    "                   7.58429925, 7.66052273, 7.70496678, 7.62830108, 7.55239822,\n",
    "                   7.4772506 , 7.40285071, 7.32919112, 7.25626445, 7.18406341,\n",
    "                   7.11258079, 7.0207356 , 7.09129517, 7.16256387, 7.23454883,\n",
    "                   7.30725726, 7.38069641, 7.45487364, 7.52979637, 7.60547208,\n",
    "                   7.68190835, 7.68351697, 6.49904846, 6.56436498, 6.63033795,\n",
    "                   6.69697395, 6.76427966, 6.8322618 , 6.90092717, 6.97028264,\n",
    "                   7.04033515, 7.11109169, 7.18255935, 6.01612613, 6.0765892 ,\n",
    "                   6.13765994, 6.19934444, 6.26164889, 6.32457951, 6.38814259,\n",
    "                   6.45234449, 6.51719163, 6.58269049, 6.64884763, 5.56908812,\n",
    "                   5.62505839, 5.68159116, 5.7386921 , 5.79636692, 5.85462137,\n",
    "                   5.9134613 , 5.97289257, 6.03292114, 6.093553  , 6.15479423,\n",
    "                   5.155268  , 5.2070793 , 5.25941132, 5.31226928, 5.36565848,\n",
    "                   5.41958424, 5.47405197, 5.5290671 , 5.58463515, 5.64076167,\n",
    "                   5.69745227])\n",
    "\n",
    "\n",
    "# Here begins hackery.\n",
    "def _gen_one_data(_, gen_one_data):\n",
    "    # See the comments in BatchData.from_func\n",
    "    return gen_one_data()\n",
    "\n",
    "\n",
    "class BatchData:\n",
    "    \"\"\"Wrapper around tf.data.Dataset.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_func(gen_one_data, batch_size=1):\n",
    "        \"\"\"Takes a function :gen_one_data: which returns a generator and a\n",
    "        :batch_size:, and returns a function which returns a tf.data.Dataset \n",
    "        producing batches of that size.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the function once so we know what its size and type is.\n",
    "        X, y = gen_one_data()\n",
    "        Xdtype = X.dtype\n",
    "        ydtype = y.dtype\n",
    "        X_batch_shape = (batch_size, *X.shape)\n",
    "        y_batch_shape = (batch_size, *y.shape)\n",
    "        \n",
    "        # So this function's implementation probably needs a little explaning.\n",
    "        # Generating data is _slow_, because in general we have to use Python\n",
    "        # to do so. So we have to use tf.py_func to feed that into TensorFlow,\n",
    "        # which in turn only runs its function inside the one interpreter.\n",
    "        # So in order to achieve speedup via multiprocessing, we have to do\n",
    "        # multiprocessing the Python way rather than the TensorFlow way.\n",
    "        pool = mp.Pool(processes=None)\n",
    "        \n",
    "        # Now we have this strange looking partial of a global function. When\n",
    "        # we come to generate our data later, this is the function that we'll\n",
    "        # be calling. It's a global function because the default Python\n",
    "        # multiprocessing package is only capable of pickling top-level\n",
    "        # functions. It then has to be a partial of this function so that we\n",
    "        # can pass it gen_one_data, i.e. the function that we're actually\n",
    "        # calling. The redundant _ argument in _gen_one_data is the the value\n",
    "        # from the iterable (defined next), which is necessary to tell the\n",
    "        # multiprocessing map how many times we want to call the function.\n",
    "        gen_data = ft.partial(_gen_one_data, gen_one_data=gen_one_data)\n",
    "        # We've defined both gen_data and batch_list here so that\n",
    "        # gen_batch_data, below, doesn't need to recreate them each time.\n",
    "        # Although to be honest this is probably one of those tiny unnoticable\n",
    "        # speedups.\n",
    "        batch_list = list(range(batch_size))\n",
    "        \n",
    "        # Next we wrap everything else in a _wrapper function, because it is\n",
    "        # creating Tensorflow's Datasets, and this has to be done inside the\n",
    "        # session, which doesn't start until we're inside the Estimator.\n",
    "        # (As a side note, there's no particular reason to put the above three \n",
    "        # lines of code either inside or outside of this wrapper; they happen \n",
    "        # to be outside.)\n",
    "        def _wrapper():\n",
    "            # Now we vectorize our data generation. Note that we have to do\n",
    "            # this here (and not via the Dataset.batch method), because we're\n",
    "            # doing Python multiprocessing, not TensorFlow multiprocessing:\n",
    "            # and we're using multiprocessing to generate multiple elements\n",
    "            # of a batch simulataneously.\n",
    "            def gen_batch_data():\n",
    "                results = pool.map(gen_data, batch_list)\n",
    "                X_batch = np.empty(X_batch_shape, dtype=Xdtype)\n",
    "                y_batch = np.empty(y_batch_shape, dtype=ydtype)\n",
    "                X_batch[:], y_batch[:] = zip(*results)\n",
    "                return X_batch, y_batch\n",
    "            \n",
    "            # Now wrap gen_batch_data to turn it into a generator.\n",
    "            def generator():\n",
    "                while True:\n",
    "                    yield gen_batch_data()\n",
    "            \n",
    "            # And finally produces our Dataset from this generator. Note that\n",
    "            # each element of this Dataset is itself a batch; we don't need to\n",
    "            # batch again.\n",
    "            ds = tfd.Dataset.from_generator(generator, (Xdtype, ydtype),\n",
    "                                            (X_batch_shape, y_batch_shape))\n",
    "            \n",
    "            return ds\n",
    "        return _wrapper\n",
    "\n",
    "    @staticmethod\n",
    "    def to_dataset(data):\n",
    "        \"\"\"Returns a tf.data.Dataset which endlessly repeats :data:.\"\"\"\n",
    "        # Lambda wrapper is because in order to be part of the same graph as\n",
    "        # the DNN, it has to be called later on.\n",
    "        return lambda: tfd.Dataset.from_tensors(data).repeat()\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch(gen_one_data, batch_size=1):\n",
    "        \"\"\"Takes a function :gen_one_data: which returns a generator and a\n",
    "        :batch_size:, and returns a batch of that size.\n",
    "        \"\"\"\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        for _ in range(batch_size):\n",
    "            X, y = gen_one_data()\n",
    "            X_batch.append(X)\n",
    "            y_batch.append(y)\n",
    "        return (np.array(X_batch), np.array(y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preprocessing\n",
    "\n",
    "class _Processor(tools.SubclassTrackerMixin('__name__')):\n",
    "    \"\"\"Base class for preprocessors.\"\"\"\n",
    "    \n",
    "    save_attr = []\n",
    "    checkpoint_filename = 'processor-checkpoint.ckpt'\n",
    "    \n",
    "    def __init__(self, training=True, **kwargs):\n",
    "        self._training = training\n",
    "        super(_Processor, self).__init__(**kwargs)\n",
    "        \n",
    "    def init(self):\n",
    "        \"\"\"Initialises TensorFlow variables.\"\"\"\n",
    "        self._saver = tft.Saver([getattr(self, name + '_tf') \n",
    "                                 for name in self.save_attr], \n",
    "                                allow_empty=True)\n",
    "        \n",
    "    def training(self, val):\n",
    "        \"\"\"Provides a context to set the training variable to :val:.\"\"\"\n",
    "        return tools.set_context_variables(self, ('_training',), val)\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        \"\"\"Processes the data.\"\"\"\n",
    "        # Note that y may be None during prediction; make sure transform is\n",
    "        # appropriately defined.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def inverse_transform(self, y):\n",
    "        \"\"\"Performs the inverse transform on the data.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def save(self, session, step, model_dir):\n",
    "        \"\"\"Saves the processor to a file in the directory :model_dir:. The argument\n",
    "        :step: is logged out to specify at what global step this was performed.\"\"\"\n",
    "        file_loc = model_dir + '/processor/' + self.checkpoint_filename\n",
    "        self._saver.save(session, file_loc, global_step=step)\n",
    "        self._sync_variables(session)\n",
    "        tflog.info('Saving processor checkpoint for {} into {}'.format(step, file_loc))\n",
    "        \n",
    "    def load(self, session, model_dir):\n",
    "        \"\"\"Sets the processor's variables to what is specified in the save file\n",
    "        located in the directory :model_dir:.\n",
    "        \"\"\"\n",
    "        file_loc = model_dir + '/processor/' + self.checkpoint_filename\n",
    "        try:\n",
    "            self._saver.restore(session, file_loc)\n",
    "        except (tfer.NotFoundError, tfer.InvalidArgumentError):\n",
    "            # NotFoundError caused by the file not existing.\n",
    "            # InvalidArgumentError caused by the folder not existing.\n",
    "            tflog.info(\"No processor checkpoint file {} found.\".format(file_loc))\n",
    "        else:\n",
    "            tflog.info(\"Restoring processor parameters from {}\".format(file_loc))\n",
    "            self._sync_variables(session)\n",
    "            \n",
    "    def _sync_variables(self, session):\n",
    "        \"\"\"We have variables both as TensorFlow Variables, and as regular python\n",
    "        variables. In general we'll want to access the values both during a\n",
    "        TensorFlow Session, and after it has been run. So for simplicity we keep\n",
    "        two sets of variables and sync between them.\"\"\"\n",
    "        \n",
    "        for name in self.save_attr:\n",
    "            tf_variable = getattr(self, name + '_tf')\n",
    "            value = tf_variable.eval(session=session)\n",
    "            setattr(self, name, value)\n",
    "            \n",
    "            \n",
    "    \n",
    "class IdentityProcessor(_Processor):\n",
    "    \"\"\"Performs no processing.\"\"\"\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        return X, y\n",
    "    \n",
    "    def inverse_transform(self, y):\n",
    "        return y\n",
    "  \n",
    "    \n",
    "class ScaleOverall(_Processor):\n",
    "    \"\"\"Scales data to between -1 and 1. Scaling is done across all batches.\"\"\"\n",
    "    \n",
    "    save_attr = ['mean', 'extent', 'momentum', '_started']\n",
    "    \n",
    "    def __init__(self, momentum=0.99, **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.mean = 0.0\n",
    "        self.extent = 1.0\n",
    "        self._started = False\n",
    "        super(ScaleDataOverall, self).__init__(**kwargs)\n",
    "        \n",
    "    def init(self):\n",
    "        self.momentum_tf = tf.Variable(self.momentum, trainable=False, dtype=tf.float64)\n",
    "        self.mean_tf = tf.Variable(self.mean, trainable=False, dtype=tf.float64)\n",
    "        self.extent_tf = tf.Variable(self.extent, trainable=False, dtype=tf.float64)\n",
    "        self._started_tf = tf.Variable(self._started, trainable=False)\n",
    "        super(NormalisationOverall, self).init()\n",
    "        \n",
    "    def transform(self, X, y):\n",
    "        def first_time():\n",
    "            self._started_tf.assign(True)\n",
    "            mean = tf.reduce_mean(X)\n",
    "            extent = tf.reduce_max(tf.abs(X - mean))\n",
    "            self.mean_tf.assign(X_mean)\n",
    "            self.extent_tf.assign(X_extent)\n",
    "        \n",
    "        def later_times():\n",
    "            mean = tf.reduce_mean(X)\n",
    "            extent = tf.reduce_max(tf.abs(X - mean))\n",
    "            self.mean_tf.assign(self.mean_tf * self.momentum_tf + mean * (1 - self.momentum_tf))\n",
    "            self.extent_tf.assign(self.extent_tf * self.momentum_tf + extent * (1 - self.momentum_tf))\n",
    "        \n",
    "        if self._training:\n",
    "            mean, extent = tf.cond(tf.equal(self._started_tf, False), first_time, later_times)\n",
    "        else:\n",
    "            mean, extent = self.mean_tf, self.extent_tf\n",
    "        \n",
    "        X_scaled = (X - mean) / extent\n",
    "        y_scaled =  None if y is None else (y - mean) / extent\n",
    "        return X_scaled, y_scaled\n",
    "    \n",
    "    def inverse_transform(self, y):\n",
    "        return (y * self.extent) + self.mean\n",
    "    \n",
    "    \n",
    "class NormalisationOverall(_Processor):\n",
    "    \"\"\"Normalises inputs by subtracting mean and dividing by standard deviation.\n",
    "    Scaling is done across all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    save_attr = ['mean', 'stddev', 'momentum', '_started']\n",
    "    \n",
    "    def __init__(self, momentum=0.99, **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.mean = 0.0\n",
    "        self.stddev = 1.0\n",
    "        self._started = False\n",
    "        super(NormalisationOverall, self).__init__(**kwargs)\n",
    "        \n",
    "    def init(self):\n",
    "        self.momentum_tf = tf.Variable(self.momentum, trainable=False, dtype=tf.float64)\n",
    "        self.mean_tf = tf.Variable(self.mean, trainable=False, dtype=tf.float64)\n",
    "        self.stddev_tf = tf.Variable(self.stddev, trainable=False, dtype=tf.float64)\n",
    "        self._started_tf = tf.Variable(self._started, trainable=False)\n",
    "        super(NormalisationOverall, self).init()\n",
    "        \n",
    "    def transform(self, X, y):\n",
    "        def first_time():\n",
    "            self._started_tf.assign(True)\n",
    "            mean = tf.reduce_mean(X)\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(X - mean)))\n",
    "            m = self.mean_tf.assign(mean)\n",
    "            s = self.stddev_tf.assign(stddev)\n",
    "            return m, s\n",
    "        \n",
    "        def later_times():\n",
    "            mean = tf.reduce_mean(X)\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(X - mean)))\n",
    "            m = self.mean_tf.assign(self.mean_tf * self.momentum_tf + mean * (1 - self.momentum_tf))\n",
    "            s = self.stddev_tf.assign(self.stddev_tf * self.momentum_tf + stddev * (1 - self.momentum_tf))\n",
    "            return m, s\n",
    "        \n",
    "        if self._training:\n",
    "            mean, stddev = tf.cond(tf.equal(self._started_tf, False), first_time, later_times)\n",
    "        else:\n",
    "            mean, stddev = self.mean_tf, self.stddev_tf\n",
    "            mean = tf.Print(mean, [mean], 'mean: ')\n",
    "            stddev = tf.Print(stddev, [stddev], 'stddev: ')\n",
    "            \n",
    "        X_scaled = (X - mean) / stddev\n",
    "        y_scaled = None if y is None else (y - mean) / stddev\n",
    "        return X_scaled, y_scaled\n",
    "    \n",
    "    def inverse_transform(self, y):\n",
    "        return (y * self.stddev) + self.mean\n",
    "    \n",
    "    \n",
    "### Hooks\n",
    "\n",
    "class _ProcessorSavingHook(tft.SessionRunHook):\n",
    "    \"\"\"Saves the processor data.\"\"\"\n",
    "    # Adapted from the source code for tf.train.CheckpointSaverHook\n",
    "    \n",
    "    def __init__(self, processor, model_dir, save_secs=600, \n",
    "                 save_steps=None, **kwargs):\n",
    "        self.processor = processor\n",
    "        self.model_dir = model_dir\n",
    "        self._timer = tft.SecondOrStepTimer(every_secs=save_secs,\n",
    "                                            every_steps=save_steps)\n",
    "        self._global_step_tensor = None\n",
    "        super(_ProcessorSavingHook, self).__init__(**kwargs)\n",
    "    \n",
    "    def begin(self):\n",
    "        self._global_step_tensor = tft.get_global_step()\n",
    "        \n",
    "    def after_create_session(self, session, coord):\n",
    "        global_step = session.run(self._global_step_tensor)\n",
    "        self._save(session, global_step)\n",
    "        self._timer.update_last_triggered_step(global_step)\n",
    "        \n",
    "    def before_run(self, run_context):\n",
    "        return tft.SessionRunArgs(self._global_step_tensor)\n",
    "        \n",
    "    def after_run(self, run_context, run_values):\n",
    "        stale_global_step = run_values.results\n",
    "        if self._timer.should_trigger_for_step(stale_global_step + 1):\n",
    "            global_step = run_context.session.run(self._global_step_tensor)\n",
    "            if self._timer.should_trigger_for_step(global_step):\n",
    "                self._timer.update_last_triggered_step(global_step)\n",
    "                self._save(run_context.session, global_step)\n",
    "            \n",
    "    def end(self, session):\n",
    "        last_step = session.run(self._global_step_tensor)\n",
    "        if last_step != self._timer.last_triggered_step():\n",
    "            self._save(session, last_step)\n",
    "        \n",
    "    def _save(self, session, step):\n",
    "        self.processor.save(session, step, self.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DNN Construction via code\n",
    "\n",
    "# Keras-inspired nice interface, just without the slow speed and lack of \n",
    "# multicore functionality of Keras...\n",
    "# (Plus it allows us to integrate our preprocessing)\n",
    "\n",
    "class Sequential:\n",
    "    \"\"\"Defines a neural network. Expected usage is roughly:\n",
    "    \n",
    "    >>> model = Sequential()\n",
    "    >>> model.add(tf.layers.Dense(units=100, activation=tf.nn.relu))\n",
    "    >>> model.add_train(tf.layers.Dropout(rate=0.4))\n",
    "    >>> model.add(tf.layers.Dense(units=50, activation=tf.nn.relu))\n",
    "    >>> model.add_train(tf.layers.Dropout(rate=0.4))\n",
    "    >>> model.add(tf.layers.Dense(units=10, activation=tf.nn.relu))\n",
    "    \n",
    "    to define the neural network in the abstract (note that the last dense layer\n",
    "    are treated as the logits), followed by:\n",
    "    \n",
    "    >>> dnn = model.compile()\n",
    "    \n",
    "    to actually create it in TensorFlow. Here, 'dnn' is a tf.Estimator, so may\n",
    "    be used like:\n",
    "    \n",
    "    >>> dnn.train(...)\n",
    "    >>> dnn.predict(...)\n",
    "    >>> dnn.evaluate(...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Creates a Sequential. See Sequential.__doc__ for more info.\"\"\"\n",
    "        self._layer_funcs = []\n",
    "        self._layer_train = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        \"\"\"Add a layer to the network.\n",
    "        \"\"\"\n",
    "        self._layer_funcs.append(layer)\n",
    "        self._layer_train.append(False)\n",
    "        \n",
    "    def add_train(self, layer):\n",
    "        \"\"\"Add a layer to the network which needs to know if the network is in\n",
    "        training or not.\n",
    "        \"\"\"\n",
    "        self.add(layer)\n",
    "        self._layer_train[-1] = True\n",
    "        \n",
    "    def compile(self, optimizer=None, loss_fn=tflo.mean_squared_error, \n",
    "                model_dir=None, gradient_clip=None, processor=None, **kwargs):\n",
    "        \"\"\"Takes its abstract neural network definition and compiles it into a\n",
    "        tf.estimator.Estimator.\n",
    "        \n",
    "        May be given an :optimizer:, defaulting to tf.train.AdamOptimizer().\n",
    "        May be given a :loss_fn:, defaulting to tf.losses.mean_squared_error.\n",
    "        May be given a :gradient_clip:, defaulting to no clipping.\n",
    "        May be given a :processor:, which will be saved and loaded.\n",
    "        \n",
    "        Any additional kwargs are passed into the creation of the\n",
    "        tf.estimator.Estimator.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probably shouldn't use the same optimizer instance every time? Hence\n",
    "        # this.\n",
    "        if optimizer is None:\n",
    "            optimizer = tft.AdamOptimizer()\n",
    "            \n",
    "        if processor is None:\n",
    "            processor = IdentityProcessor()\n",
    "            \n",
    "        def model_fn(features, labels, mode):\n",
    "            # Create processor variables\n",
    "            processor.init()\n",
    "            processor.load()\n",
    "            \n",
    "            # Will be called once the session starts up, in order to load any\n",
    "            # existing saved values for the processor variables\n",
    "#             def init_fn():\n",
    "#                 if model_dir is not None:\n",
    "#                     processor.load(tf.get_default_session(), model_dir)\n",
    "#                 return tf.constant(True)\n",
    "#             scaffold = tft.Scaffold(local_init_op=tf.py_func(init_fn, [], tf.bool))\n",
    "            \n",
    "            # Apply any preprocessing to the features and labels\n",
    "            features, labels = processor.transform(features, labels)\n",
    "            \n",
    "            # First layer is the feature inputs.\n",
    "            layers = [features]\n",
    "            \n",
    "            for prev_layer, layer_func, train in zip(layers, self._layer_funcs, \n",
    "                                                     self._layer_train):\n",
    "                if train:\n",
    "                    layer = layer_func(inputs=prev_layer, \n",
    "                                       training=mode == tfe.ModeKeys.TRAIN)\n",
    "                else:\n",
    "                    layer = layer_func(inputs=prev_layer)\n",
    "                    \n",
    "                # Deliberately using the generator nature of zip to add elements\n",
    "                # to the layers list as we're iterating through it.\n",
    "                # https://media.giphy.com/media/3oz8xtBx06mcZWoNJm/giphy.gif\n",
    "                layers.append(layer)\n",
    "                \n",
    "            logits = layers[-1]\n",
    "            \n",
    "            if mode == tfe.ModeKeys.PREDICT:\n",
    "                return tfe.EstimatorSpec(mode=mode, predictions=logits, scaffold=scaffold)\n",
    "            \n",
    "            loss = loss_fn(labels, logits)\n",
    "\n",
    "            if mode == tfe.ModeKeys.TRAIN:\n",
    "                g_step = tft.get_global_step()\n",
    "                if gradient_clip is None:\n",
    "                    train_op = optimizer.minimize(loss=loss, global_step=g_step)\n",
    "                else:\n",
    "                    # Perform Gradient clipping\n",
    "                    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "                    with tf.control_dependencies(update_ops):\n",
    "                        gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "#                         gradients0 = tf.Print(gradients[0], [tf.global_norm(gradients)], 'Global norm: ')\n",
    "#                         gradients = tuple([gradients0, *gradients[1:]])\n",
    "                        gradients, _ = tf.clip_by_global_norm(gradients, \n",
    "                                                              gradient_clip)\n",
    "                        train_op = optimizer.apply_gradients(zip(gradients, \n",
    "                                                                 variables),\n",
    "                                                             global_step=g_step)\n",
    "                training_hooks = [] if model_dir is None else [_ProcessorSavingHook(processor, model_dir)]\n",
    "                return tfe.EstimatorSpec(mode=mode, loss=loss, train_op=train_op,\n",
    "                                         scaffold=scaffold,\n",
    "                                         training_hooks=training_hooks)\n",
    "            \n",
    "            if mode == tfe.ModeKeys.EVAL:\n",
    "                return tfe.EstimatorSpec(mode=mode, loss=loss, scaffold=scaffold)\n",
    "            \n",
    "            raise RuntimeError(\"mode '{}' not understood\".format(mode))\n",
    "                \n",
    "        return tfe.Estimator(model_fn=model_fn, model_dir=model_dir, **kwargs)\n",
    "    \n",
    "    \n",
    "def model_dir_str(model_dir, hidden_units, logits, processor, activation, \n",
    "                  uuid=None):\n",
    "    \"\"\"Returns a string for the model directory describing the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    layer_counter = [(k, sum(1 for _ in g)) for k, g in it.groupby(hidden_units)]\n",
    "    for layer_size, layer_repeat in layer_counter:\n",
    "        if layer_repeat == 1:\n",
    "            model_dir += '{}_'.format(layer_size)\n",
    "        else:\n",
    "            model_dir += '{}x{}_'.format(layer_size, layer_repeat)\n",
    "    model_dir += '{}__'.format(logits)\n",
    "    model_dir += processor.__class__.__name__\n",
    "    \n",
    "    if isinstance(activation, ft.partial):\n",
    "        activation_fn = activation.func\n",
    "        alpha = str(activation.keywords['alpha']).replace('.', '')\n",
    "    else:\n",
    "        activation_fn = activation\n",
    "        alpha = '02'\n",
    "        \n",
    "    model_dir += '_' + activation_fn.__name__.replace('_', '')\n",
    "    if activation_fn is tf.nn.leaky_relu:\n",
    "        model_dir += alpha\n",
    "\n",
    "    if uuid not in (None, ''):\n",
    "        model_dir += '_' + str(uuid)\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DNN construction from folder\n",
    "\n",
    "def _dnn_hyperparameters_from_dir(dir_name):\n",
    "    \"\"\"Creates DNN hyperparameters from the name of the directory of the DNN.\n",
    "    \"\"\"\n",
    "    \n",
    "    dnn_details = {}\n",
    "\n",
    "    units, rest = dir_name.split('__')\n",
    "    units = units.split('_')\n",
    "    rest = rest.split('_')\n",
    "    \n",
    "    all_units = []\n",
    "    for unit in units:\n",
    "        if 'x' in unit:\n",
    "            unit_size, unit_repeat = unit.split('x')\n",
    "            unit_size, unit_repeat = int(unit_size), int(unit_repeat)\n",
    "            all_units.extend([unit_size for _ in range(unit_repeat)])\n",
    "        else:\n",
    "            all_units.append(int(unit))\n",
    "    dnn_details['hidden_units'] = all_units[:-1]\n",
    "    dnn_details['logits'] = all_units[-1]\n",
    "    \n",
    "    processor_name = rest[0]\n",
    "    processor_class = _Processor.find_subclass(processor_name)\n",
    "    dnn_details['processor'] = processor_class()\n",
    "    dnn_details['batch_norm'] = False\n",
    "    \n",
    "    activation_name = rest[1].lower()\n",
    "    \n",
    "    # Not a great way to do this inversion, admittedly\n",
    "    if activation_name[:9] == 'leakyrelu':\n",
    "        alpha = float(str(activation_name[9]) + '.' + str(activation_name[10:]))\n",
    "        dnn_details['activation'] = ft.partial(tf.nn.leaky_relu, alpha=alpha)\n",
    "    else:\n",
    "        try:\n",
    "            activation_fn = getattr(tf.nn, activation_name)\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"Activation '{}' not understood.\".format(activation_name))\n",
    "        else:\n",
    "            dnn_details['activation'] = activation_fn\n",
    "        \n",
    "    remaining = rest[2:]\n",
    "    if len(remaining) == 0:\n",
    "        uuid = None\n",
    "    elif len(remaining) == 1:\n",
    "        uuid = remaining[0]\n",
    "    else:\n",
    "        raise RuntimeError(\"Bad dir_name string '{}'. Too many remaining \"\n",
    "                           \"arguments: {}\".format(dir_name, remaining))\n",
    "        \n",
    "    return dnn_details, uuid\n",
    "\n",
    "\n",
    "def dnn_factory_from_model_dir(model_dir, **kwargs):\n",
    "    \"\"\"Creates a DNN from the :model_dir: argument. Any additional keyword\n",
    "    arguments provided override the details of the DNN found.\"\"\"\n",
    "    \n",
    "    if model_dir[-1] in ('/', '\\\\'):\n",
    "        model_dir = model_dir[:-1]\n",
    "    model_dir_split = tools.split(['/', '\\\\'], model_dir)\n",
    "    dir_name = model_dir_split[-1]\n",
    "    # I suspect that we should be able to restore the DNN just from the\n",
    "    # information saved in the model directory, without needing to know\n",
    "    # its structure from the directory name...\n",
    "    dnn_details, uuid = _dnn_hyperparameters_from_dir(dir_name)\n",
    "    dnn_details.update(kwargs)\n",
    "    dnn_factory = DNNFactory(model_dir=model_dir, **dnn_details)\n",
    "    return dnn_factory\n",
    "\n",
    "\n",
    "def dnn_factories_from_dir(dir_, exclude_start=('.',), exclude_end=(), \n",
    "                           exclude_in=(), **kwargs):\n",
    "    \"\"\"Creates multiple DNNs and processors from a directory containing the\n",
    "    directories for multiple DNNs and processors.\n",
    "    \n",
    "    Its arguments :exclude_start:, :exclude_end:, :exclude_in: are each\n",
    "    tuples which allow for excluding particular models, if their model \n",
    "    directories start, end, or include any of the strings specified\n",
    "    in each tuple respectively.\n",
    "    \n",
    "    Essentially just a wrapper around dnn_factory_from_model_dir, to run it\n",
    "    multiple times. It will forward any additional keyword arguments onto\n",
    "    each call of dnn_factory_from_model_dir.\n",
    "    \"\"\"\n",
    "    \n",
    "    subdirectories = sorted(next(os.walk(dir_))[1])\n",
    "    if dir_[-1] in ('/', '\\\\'):\n",
    "        dir_ = dir_[:-1]\n",
    "    dnn_factories = []\n",
    "    names = []\n",
    "    \n",
    "    for subdir in subdirectories:\n",
    "        if any(subdir.startswith(ex) for ex in exclude_start):\n",
    "            tflog.info(\"Excluding '{}' based on start.\".format(subdir))\n",
    "            continue\n",
    "        if any(subdir.endswith(ex) for ex in exclude_end):\n",
    "            tflog.info(\"Excluding '{}' based on end.\".format(subdir))\n",
    "            continue\n",
    "        if any(ex in subdir for ex in exclude_in):\n",
    "            tflog.info(\"Excluding '{}' based on containment.\".format(subdir))\n",
    "            continue\n",
    "            \n",
    "        model_dir = dir_ + '/' + subdir\n",
    "        try:\n",
    "            dnn_factory = dnn_factory_from_model_dir(model_dir, **kwargs)\n",
    "        except (FileNotFoundError, RuntimeError) as e:\n",
    "            tflog.info(\"Could not load DNN from '{}'. Error message: '{}'\"\n",
    "                       .format(subdir, e))\n",
    "        else:\n",
    "            dnn_factories.append(dnn_factory)\n",
    "            names.append(subdir)\n",
    "            \n",
    "    return dnn_factories, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simpler interpolation methods\n",
    "# Useful to give a baseline to compare the neural network models against.\n",
    "\n",
    "class _InterpolatorBase:\n",
    "    \"\"\"Base class for performing predictions based on just the input. Subclasses\n",
    "    are expected to provide a predict_single classmethod specifying their\n",
    "    predictions.\n",
    "    \n",
    "    Its predict and evaluate methods are designed to resemble that of\n",
    "    tf.estimator.Estimator's, so that we can call them in the same way. (We don't\n",
    "    actually inherit from tf.estimator.Estimator because none of what these \n",
    "    classes use TensorFlow, so messing around with model functions and \n",
    "    EstimatorSpecs is just unnecessary faff and overhead.)\n",
    "    \n",
    "    WARNING: All subclasses must expect no preprocessing, i.e. must use\n",
    "    IdentityProcessor() as their preprocessing. This is because preprocessing is\n",
    "    done in TensorFlow, which, of course, this class is explicitly about not \n",
    "    using... if any preprocessing is necessary then subclasses must implement it\n",
    "    themselves as part of their predict_single method.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _index_tol(cg, point, tol=0.001):\n",
    "        \"\"\"Searches through a list of 2-tuples, :cg:, to find the first element \n",
    "        which is within tolerance :tol: of :point:. Essentially the index method\n",
    "        for lists, except this one makes sense for high precision floating point\n",
    "        numbers.\n",
    "        \"\"\"\n",
    "        \n",
    "        t, x = point\n",
    "        for i, element in enumerate(cg):\n",
    "            t2, x2 = element\n",
    "            if max(np.abs(t - t2), np.abs(x - x2)) < tol:\n",
    "                return i\n",
    "        raise ValueError('{} is not in {}'.format(point, type(cg)))\n",
    "        \n",
    "    def _prepare(self, Xi):\n",
    "        \"\"\"Performs any necessary preparations on the data :Xi: before making \n",
    "        predictions.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _interp(self, Xi, point):\n",
    "        \"\"\"Helper function for performing interpolation on a coarse\n",
    "        grid :Xi:, giving the value of the interpolation at :point:.\n",
    "        \n",
    "        The spacing of the grid is known from the global hyperparameters\n",
    "        defining the coarse grid size, whilst it isn't necessary to know its\n",
    "        location.\n",
    "        \n",
    "        The argument :point: should be scaled to the grid size, i.e.\n",
    "        coarse_grid_sep.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict_single(self, Xi, y):\n",
    "        \"\"\"Makes a prediction corresponding to input feature :Xi:.\n",
    "        \n",
    "        It is given the true result :y:. Not to cheat and return perfect\n",
    "        results, but to determine its shape etc.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict(self, input_fn, yield_single_examples=False):\n",
    "        \"\"\"The argument :input_fn: should probably be a lambda wrapper around\n",
    "        the result of BatchData.test.\n",
    "        \n",
    "        The argument :yield_single_examples: is there for compatibility with the\n",
    "        interface for the usual TF Estimators and is ignored.\n",
    "        \"\"\"\n",
    "        \n",
    "        returnval = []\n",
    "        X, y = input_fn()\n",
    "        \n",
    "        for Xi in X:\n",
    "            returnval.append(self.predict_single(Xi, y))\n",
    "            \n",
    "        returnval = np.array(returnval)\n",
    "        while True:\n",
    "            yield returnval\n",
    "\n",
    "\n",
    "class BilinearInterpMixin(_InterpolatorBase):\n",
    "    \"\"\"Mixin to help perform bilinear interpolation.\"\"\"\n",
    "        \n",
    "    def _interp(self, Xi, point):        \n",
    "        # The actual t, x values for the grid don't matter from this point \n",
    "        # onwards; so this is just a translation from wherever X was actually \n",
    "        # calculated. So WLOG assume it was around 0.\n",
    "        cg = coarse_grid((0, 0))\n",
    "        t, x = point\n",
    "        \n",
    "        # The grid points nearest :point:.\n",
    "        t_below = tools.round_mult(t, coarse_grid_sep.t, 'down')\n",
    "        t_above = tools.round_mult(t, coarse_grid_sep.t, 'up')\n",
    "        x_below = tools.round_mult(x, coarse_grid_sep.x, 'down')\n",
    "        x_above = tools.round_mult(x, coarse_grid_sep.x, 'up')\n",
    "        \n",
    "        # The value of :Xi: at those grid points.\n",
    "        t_b_x_b = Xi[self._index_tol(cg, (t_below, x_below))]\n",
    "        t_a_x_b = Xi[self._index_tol(cg, (t_above, x_below))]\n",
    "        t_b_x_a = Xi[self._index_tol(cg, (t_below, x_above))]\n",
    "        t_a_x_a = Xi[self._index_tol(cg, (t_above, x_above))]\n",
    "        \n",
    "        # Shift the t, x values to be relative to the bottom-left point of the\n",
    "        # grid square in which (t, x) lies.\n",
    "        t_scale = (t % coarse_grid_sep.t) / coarse_grid_sep.t\n",
    "        x_scale = (x % coarse_grid_sep.x) / coarse_grid_sep.x\n",
    "        \n",
    "        # Bilinear interpolation\n",
    "        returnval = (1 - t_scale) * (1 - x_scale) * t_b_x_b\n",
    "        returnval += t_scale * (1 - x_scale) * t_a_x_b\n",
    "        returnval += (1 - t_scale) * x_scale * t_b_x_a\n",
    "        returnval += t_scale * x_scale * t_a_x_a\n",
    "        \n",
    "        return returnval\n",
    "    \n",
    "    \n",
    "class PolyInterpMixin(_InterpolatorBase):\n",
    "    \"\"\"Mixin to help perform polynomial interpolation.\"\"\"\n",
    "    \n",
    "    def __init__(self, poly_deg, *args, **kwargs):\n",
    "        self.poly_deg = poly_deg\n",
    "        self._poly_coefs = None\n",
    "        super(PolyInterpBase, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def poly(self, point):\n",
    "        \"\"\"Interprets its currently stored polynomial coefficients as a \n",
    "        polynomial, and evaluates them at the specified point.\"\"\"\n",
    "        \n",
    "        if self._poly_coefs is None:\n",
    "            raise RuntimeError('Must run _prepare first!')\n",
    "        \n",
    "        t, x = point\n",
    "        coefs = iter(self._poly_coefs)\n",
    "\n",
    "        result = next(coefs)  # Intercept, i.e. constant term\n",
    "        for power in range(1, self.poly_deg + 1):\n",
    "            for x_power in range(0, power + 1):\n",
    "                t_power = power - x_power\n",
    "                coef = next(coefs)\n",
    "                result += coef * (t ** t_power) * (x ** x_power)\n",
    "        try:\n",
    "            next_coef = next(coefs)\n",
    "        except StopIteration:\n",
    "            return result\n",
    "        else:\n",
    "            raise RuntimeError('coef_: {coef_}, poly_deg: {poly_deg}, '\n",
    "                               'coef that shouldn\\'t exist: {next_coef}'\n",
    "                               .format(coef_=coef_, \n",
    "                                       poly_deg=self.poly_deg, \n",
    "                                       next_coef=next_coef))\n",
    "    \n",
    "    def _prepare(self, Xi):\n",
    "        poly_features = skpr.PolynomialFeatures(degree=self.poly_deg, \n",
    "                                                include_bias=True)\n",
    "        lin_reg = sklm.LinearRegression(fit_intercept=False)\n",
    "        poly_pipe = skpi.Pipeline([('pf', poly_features), ('lr', lin_reg)])\n",
    "        \n",
    "        # The actual t, x values for the grid don't matter from this point \n",
    "        # onwards; so this is just a translation from wherever X was actually \n",
    "        # calculated. So WLOG assume it was around 0.\n",
    "        cg = coarse_grid((0, 0))\n",
    "        poly_pipe.fit(cg, Xi)\n",
    "        self._poly_coefs = poly_pipe.named_steps['lr'].coef_\n",
    "        \n",
    "    \n",
    "    def _interp(self, Xi, point):\n",
    "        return self.poly(point)\n",
    "    \n",
    "    \n",
    "class NearestInterpMixin(_InterpolatorBase):\n",
    "    \"\"\"Mixin to help perform nearest-neighbour interpolation.\"\"\"\n",
    "        \n",
    "    def _interp(self, Xi, point):        \n",
    "        # The actual t, x values for the grid don't matter from this point \n",
    "        # onwards; so this is just a translation from wherever X was actually \n",
    "        # calculated. So WLOG assume it was around 0.\n",
    "        cg = coarse_grid((0, 0))\n",
    "        t, x = point\n",
    "        \n",
    "        # The grid point nearest :point:.\n",
    "        t_nearest = tools.round_mult(t, coarse_grid_sep.t, 'round')\n",
    "        x_nearest = tools.round_mult(x, coarse_grid_sep.x, 'round')\n",
    "        \n",
    "        # The value of :Xi: at those grid points.\n",
    "        t_n_x_n = Xi[self._index_tol(cg, (t_nearest, x_nearest))]\n",
    "\n",
    "        return t_n_x_n\n",
    "    \n",
    "    \n",
    "class FineGridInterpolator(_InterpolatorBase):\n",
    "    \"\"\"Provides the predict_single function for predictions on a fine grid.\n",
    "    \n",
    "    Requires the _prepare and _interp methods provided by one of the mixins\n",
    "    above.\"\"\"\n",
    "    \n",
    "    def predict_single(self, Xi, y):\n",
    "        returnval = []\n",
    "        # Translation doesn't matter at this point so WLOG the fine grid is\n",
    "        # around 0, 0. (cls._interp makes the same assumption; these assumptions\n",
    "        # must be consistent)\n",
    "        for point in fine_grid((0, 0)):\n",
    "            self._prepare(Xi)\n",
    "            returnval.append(self._interp(Xi, point))\n",
    "        return returnval\n",
    "    \n",
    "    \n",
    "class PointInterpolator(_InterpolatorBase):\n",
    "    \"\"\"Provides the predict_single function for predictions at a single point.\n",
    "    \n",
    "    Requires the _prepare and _interp methods provided by one of the mixins\n",
    "    above.\"\"\"\n",
    "    \n",
    "    def predict_single(self, Xi):\n",
    "        # Separate the location data and the grid data\n",
    "        t_offset = Xi[-2]\n",
    "        x_offset = Xi[-1]\n",
    "        Xi = Xi[:-2]\n",
    "        self._prepare(Xi)\n",
    "        # Wrapped in a list for consistency: this network just happens to only\n",
    "        # be trying to predict a single label.\n",
    "        return [self._interp(Xi, (t_offset * coarse_grid_sep.t, \n",
    "                                  x_offset * coarse_grid_sep.x))]\n",
    "    \n",
    "    \n",
    "class Perfect(_InterpolatorBase):\n",
    "    \"\"\"Regressor that cheats to always give the perfect prediction.\"\"\"\n",
    "    \n",
    "    def predict_single(self, Xi, y):\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using regressors in ensembles\n",
    "\n",
    "class RegressorAverager:\n",
    "    \"\"\"Regressor that averages the results of other regressors to make its\n",
    "    prediction. It is capable of using both TensorFlow-based regressors\n",
    "    and non-TensorFlow-based regressors as it expects regressor factories,\n",
    "    and the difference between them is handled by the factory wrapper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, regressor_factories=None, mask=None, **kwargs):\n",
    "        \"\"\"Should be passed an iterable of :regressor_factories:. It will\n",
    "        make predictions according to their average.\n",
    "        \n",
    "        May also pass a :mask: argument, which should be a tuple of bools\n",
    "        the same length as the number of regressors, specifying whether or\n",
    "        not particular regressor should be used when making predictions.\n",
    "        \"\"\"\n",
    "        self.regressor_factories = regressor_factories\n",
    "        self.mask = None\n",
    "        self.reset_mask()\n",
    "        if mask is not None:\n",
    "            self.set_mask(mask)\n",
    "        super(RegressorAverager, self).__init__(**kwargs)\n",
    "        \n",
    "    def set_mask(self, mask):\n",
    "        assert len(mask) == len(self.regressors_with_pro)\n",
    "        self.mask = tuple(mask)\n",
    "        return self  # for chaining\n",
    "        \n",
    "    def reset_mask(self):\n",
    "        self.mask = [True for _ in range(len(self.regressors_with_pro))]\n",
    "        return self  # for chaining\n",
    "        \n",
    "    def predict(self, input_fn, *args, **kwargs):\n",
    "        X, y = input_fn()\n",
    "        \n",
    "        returnval = tools.AddBase()\n",
    "        counter = 0\n",
    "        for regressor_factory, mask in zip(self.regressor_factories, self.mask):\n",
    "            if mask:\n",
    "                counter += 1\n",
    "                returnval += _eval_regressor(regressor_factory, X, y).prediction\n",
    "        returnval = returnval / counter\n",
    "        \n",
    "        while True:\n",
    "            yield returnval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regressor factories\n",
    "# TensorFlow demands that its Estimators be rebuilt before each train(...)\n",
    "# etc. call, so DNNFactory is necessary to construct it each time.\n",
    "# For a consistent interface, RegressorFactory is also provided to wrap\n",
    "# around non-TensorFlow regressors.\n",
    "\n",
    "class _RegressorFactoryBase:\n",
    "    \"\"\"Defines the interface for factories which make regressors, i.e. DNNs\n",
    "    or simple interpolators.\n",
    "    \"\"\"\n",
    "    # No good way to set abstract instance attributes (even with @property)\n",
    "    # so we just list them in comments here. (Best way would probably be to\n",
    "    # check for the attributes existence in the metaclass' __call__, which\n",
    "    # is easily more faff than it's worth.)\n",
    "    \n",
    "    # Instances should have a 'processor' attribute returning the processor\n",
    "    # for preprocessing input data to the regressor\n",
    "    \n",
    "    # Instances should have a 'use_tf' attribute returning True or False\n",
    "    # for whether the regressor uses TensorFlow\n",
    "    \n",
    "    def __call__(self):\n",
    "        # Should return the regressor itself\n",
    "        raise NotImplementedError\n",
    "        \n",
    "            \n",
    "class DNNFactory(_RegressorFactoryBase):\n",
    "    \"\"\"Shortcut for creating a simple DNN with dense, dropout and batch \n",
    "    normalization layers, and then compiling it.\n",
    "    \n",
    "    The reason its __call__ function has this class wrapper is because of\n",
    "    how TensorFlow operates: the DNN needs to be recreated before every\n",
    "    train(...), predict(...) or evaluate(...) call, so it is convenient to\n",
    "    cache the hyperparameters for the DNN in this class, and simply call\n",
    "    the class before each train/predict/evaluate call.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_units, logits, processor=None, \n",
    "                 activation=tf.nn.relu, drop_rate=0.0, \n",
    "                 drop_type='dropout', model_dir=None, log_steps=100,\n",
    "                 gradient_clip=None, batch_norm=False, \n",
    "                 kernel_initializer=tfi.truncated_normal(mean=0, stddev=0.05),\n",
    "                 compile_kwargs = None,\n",
    "                 **kwargs):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.logits = logits\n",
    "        self.processor = processor\n",
    "        self.activation = activation\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_type = drop_type\n",
    "        self.model_dir = model_dir\n",
    "        self.log_steps = log_steps\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.batch_norm = batch_norm\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.compile_kwargs = {} if compile_kwargs is None else compile_kwargs\n",
    "        self.use_tf = True\n",
    "        super(DNNFactory, self).__init__(**kwargs)\n",
    "\n",
    "    def __call__(self):\n",
    "        model = Sequential()\n",
    "        if self.batch_norm:\n",
    "            model.add_train(tfla.BatchNormalization())\n",
    "        for units in self.hidden_units:\n",
    "            model.add(tfla.Dense(units=units, activation=self.activation,\n",
    "                                 kernel_initializer=self.kernel_initializer))\n",
    "            if self.batch_norm:\n",
    "                model.add_train(tfla.BatchNormalization())\n",
    "            if self.drop_rate != 0:\n",
    "                if self.drop_type in ('normal', 'dropout'):\n",
    "                    model.add_train(tfla.Dropout(rate=self.drop_rate))\n",
    "                elif self.drop_type in ('alpha', 'alpha_dropout'):\n",
    "                    model.add_train(tfk.layers.AlphaDropout(rate=self.drop_rate))\n",
    "        model.add(tf.layers.Dense(units=self.logits, \n",
    "                                  kernel_initializer=self.kernel_initializer))\n",
    "\n",
    "        return model.compile(gradient_clip=self.gradient_clip, \n",
    "                             processor=self.processor,\n",
    "                             model_dir=self.model_dir,\n",
    "                             config=tfe.RunConfig(log_step_count_steps=self.log_steps),\n",
    "                             **self.compile_kwargs)\n",
    "    \n",
    "    \n",
    "class RegressorFactory(_RegressorFactoryBase):\n",
    "    \"\"\"Factory wrapper around any regressor which doesn't use TensorFlow; i.e.\n",
    "    the interpolators above, or RegressorAverager (whether or not\n",
    "    RegressorAverager itself is using TensorFlow-based regressors.)\"\"\"\n",
    "    \n",
    "    def __init__(self, interpolator, **kwargs):\n",
    "        self.interpolator = interpolator\n",
    "        self.processor = IdentityProcessor()\n",
    "        self.use_tf = False\n",
    "        super(RegressorFactory, self).__init__(**kwargs)\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.interpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing and evaluating regressors\n",
    "\n",
    "def _eval_regressor(regressor_factory, X, y):\n",
    "    \"\"\"Evaluates a regressor on some test data :X:, :y:.\n",
    "    \"\"\"\n",
    "    \n",
    "    regressor = regressor_factory()\n",
    "    processor = regressor_factory.processor\n",
    "    use_tf = regressor_factory.use_tf\n",
    "    \n",
    "    if use_tf:\n",
    "        data_func = BatchData.to_dataset((X, y))\n",
    "    else:\n",
    "        data_func = lambda: (X, y)\n",
    "    \n",
    "    with processor.training(False):\n",
    "        predictor = regressor.predict(input_fn=data_func,\n",
    "                                      yield_single_examples=False)\n",
    "        prediction_before_postprocessing = next(predictor)\n",
    "        prediction = processor.inverse_transform(prediction_before_postprocessing)\n",
    "        \n",
    "    diff = prediction - y\n",
    "    squared_error = np.square(diff)\n",
    "    result = tools.Object(prediction=prediction,\n",
    "                          X=X,\n",
    "                          y=y,\n",
    "                          diff=diff,\n",
    "                          average_loss=np.mean(squared_error),\n",
    "                          loss=np.sum(squared_error))\n",
    "    return result\n",
    "\n",
    "def _eval_regressors(regressor_factories, X, y):\n",
    "    \"\"\"Evaluates an iterable of regressors on some test data\n",
    "    :X:, :y:.\"\"\"\n",
    "    results = []\n",
    "    for regressor_factory in regressor_factories:\n",
    "        result = _eval_regressor(regressor_factory, X, y)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "def eval_regressor(regressor_factory, gen_one_data, batch_size=1):\n",
    "    \"\"\"Evaluates a regressor on some test data of size :batch_size:\n",
    "    generated from :gen_one_data:.\n",
    "    \"\"\"\n",
    "    X, y = BatchData.batch(gen_one_data, batch_size)\n",
    "    return _eval_regressor(regressor_factory, X, y)\n",
    "\n",
    "\n",
    "def eval_regressors(regressor_factories, gen_one_data, batch_size=1):\n",
    "    \"\"\"Evaluates an iterable of regressors on some test data of size\n",
    "    :batch_size: generated from :gen_one_data:.\n",
    "    \"\"\"\n",
    "    X, y = BatchData.batch(gen_one_data, batch_size)\n",
    "    return _eval_regressors(regressor_factories, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualising the results of regressors\n",
    "\n",
    "# Only plots fine grid style stuff at the moment\n",
    "def plot_regressors(regressor_factories, names, X, y):\n",
    "    \"\"\"Plots the results of some regressors using the given data.\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8 * len(regressor_factories)))\n",
    "    \n",
    "    results = _eval_regressors(regressor_factories, X, y)\n",
    "    \n",
    "    for i, (result, name) in enumerate(zip(results, names)):\n",
    "        ax = make_3d_ax_for_grid_plotting(fig, (len(regressor_factories), 1, i + 1))\n",
    "        grid_plot(ax, X, 'cg', '_nolegend_')\n",
    "        grid_plot(ax, result.prediction, 'fg', name)\n",
    "        ax.legend()\n",
    "        \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
