{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies:\n",
    "#  base_eq.ipynb\n",
    "#  base_plot.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import sklearn.base as skb\n",
    "import sklearn.preprocessing as skpr\n",
    "import sklearn.pipeline as skpi\n",
    "import sklearn.linear_model as sklm\n",
    "\n",
    "# https://github.com/patrick-kidger/tools\n",
    "import tools\n",
    "\n",
    "import tensorflow as tf\n",
    "tfer = tf.errors\n",
    "tfe = tf.estimator\n",
    "tfi = tf.initializers\n",
    "tfk = tf.keras\n",
    "tfla = tf.layers\n",
    "tflog = tf.logging\n",
    "tflo = tf.losses\n",
    "tft = tf.train\n",
    "\n",
    "# Convenience imports for those files running this one\n",
    "import collections as co\n",
    "import functools as ft\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid hyperparameters\n",
    "# Everything we do is on a grid\n",
    "\n",
    "# The separation between points of the fine grid\n",
    "fine_grid_sep = tools.Object(t=0.01, x=0.01)\n",
    "# The separation between points of the coarse grid\n",
    "coarse_grid_sep = tools.Object(t=0.1, x=0.1)\n",
    "# The amount of intervals in the coarse grid. Thus the coarse grid will contain\n",
    "# (num_intervals.t + 1) * (num_intervals.x + 1) elements.\n",
    "# So with num_intervals.t = 3, num_intervals.x = 3, it looks like:\n",
    "#\n",
    "# @ @ @ @\n",
    "#\n",
    "# @ @ @ @\n",
    "#\n",
    "# @ @ @ @\n",
    "#\n",
    "# @ @ @ @\n",
    "num_intervals = tools.Object(t=7, x=7)\n",
    "\n",
    "\n",
    "fine_grid_fineness = tools.Object(t=int(coarse_grid_sep.t // fine_grid_sep.t), \n",
    "                                  x=int(coarse_grid_sep.x // fine_grid_sep.x))\n",
    "coarse_grid_size = tools.Object(t=num_intervals.t * coarse_grid_sep.t,\n",
    "                                x=num_intervals.x * coarse_grid_sep.x)\n",
    "\n",
    "\n",
    "### Grids to evaluate our solution on\n",
    "\n",
    "def grid(point, grid_size, grid_fineness):\n",
    "    \"\"\"Creates a grid whose bottom left entry is at the specified :point:\n",
    "    location. The size of the overall grid may be specified via :grid_size:, and\n",
    "    the fineness of the subdivision by :grid_fineness:, both of which should be\n",
    "    of the form tools.Object(t, x). Thus the resulting grid has\n",
    "    (grid_fineness.t + 1) * (grid_fineness.x + 1) elements.\"\"\"\n",
    "    t, x = point\n",
    "    return [(t_, x_) for t_ in np.linspace(t, t + grid_size.t, \n",
    "                                           grid_fineness.t + 1)\n",
    "                     for x_ in np.linspace(x, x + grid_size.x, \n",
    "                                           grid_fineness.x + 1)]\n",
    "\n",
    "def fine_grid(point):\n",
    "    \"\"\"Creates a fine grid whose bottom left entry is at the specified :point:\n",
    "    location, with size and fineness determined by the earlier hyperparameters.\n",
    "    \"\"\"\n",
    "    return grid(point, coarse_grid_sep, fine_grid_fineness)\n",
    "\n",
    "def coarse_grid(point):\n",
    "    \"\"\"Creates a coarse grid for which the bottom left entry of its middle\n",
    "    square is as the specified :t:, :x: location, with size and fineness\n",
    "    determined by the earlier hyperparameters.\n",
    "    \"\"\"\n",
    "    left_intervals_t = np.floor((num_intervals.t - 1) / 2)\n",
    "    left_intervals_x = np.floor((num_intervals.x - 1) / 2)\n",
    "    \n",
    "    left_amount_t = left_intervals_t * coarse_grid_sep.t\n",
    "    left_amount_x = left_intervals_x * coarse_grid_sep.x\n",
    "    \n",
    "    t, x = point\n",
    "    bottomleft_point = (t - left_amount_t, x - left_amount_x)\n",
    "    return grid(bottomleft_point, coarse_grid_size, num_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data generation\n",
    "\n",
    "def gen_one_peakon():\n",
    "    \"\"\"Returns a random peakon and a random location.\"\"\"\n",
    "    # Random solution to the CH equation\n",
    "    c = np.random.uniform(3, 10)\n",
    "    peakon = Peakon(c=c)\n",
    "    # Random location near the peak\n",
    "    t = np.random.uniform(0, 10)\n",
    "    x = c * t + np.random.uniform(-2, 2)\n",
    "    return (t, x), peakon\n",
    "\n",
    "def gen_two_peakon():\n",
    "    \"\"\"Returns a random two peakon solution, and a random location.\"\"\"\n",
    "    # Random solution to the CH equation\n",
    "    p1 = np.random.uniform(3, 10)\n",
    "    p2 = np.random.uniform(3, 10)\n",
    "    x1 = np.random.uniform(0, 3)\n",
    "    x2 = np.random.uniform(3.001, 6)\n",
    "    twopeakon = TwoPeakon(x1, x2, p1, p2)\n",
    "    # Random location near both of the peaks\n",
    "    t = np.random.uniform(0, 0.5)\n",
    "    left = min(x1 - 0.5 + p1 * t, x2 - 0.5 + p2 * t)\n",
    "    right = max(x1 + 0.5 + p1 * t, x2 + 0.5 + p2 * t)\n",
    "    middle = (right + left) / 2\n",
    "    semidist = (right - left) / 2\n",
    "    x = middle + semidist * np.random.uniform(-1, 1) ** 3\n",
    "    return (t, x), twopeakon\n",
    "\n",
    "def sol_on_grid(point, solution):\n",
    "    \"\"\"Returns the values of the :solution: on fine and coarse grids around the\n",
    "    specified :point:.\n",
    "    \"\"\"\n",
    "    # Grids at the location\n",
    "    cg = coarse_grid(point)\n",
    "    fg = fine_grid(point)\n",
    "    # Features: the solution on the coarse grid\n",
    "    X = solution.on_grid(cg)\n",
    "    # Labels: the solution on the fine grid\n",
    "    y = solution.on_grid(fg)\n",
    "    return X, y\n",
    "\n",
    "def sol_at_point(point, solution):\n",
    "    \"\"\"Returns the values of the :solution: on a coarse grid and at a random\n",
    "    point near the specified :point:.\n",
    "    \"\"\"\n",
    "    \n",
    "    cg = coarse_grid(point)\n",
    "    \n",
    "    # Random offset from the random location that we ask for predictions at. The\n",
    "    # distribution is asymmetric because we're moving relative to :point:, which\n",
    "    # is in the _bottom left_ of the central cell of the coarse grid. The asymmetric\n",
    "    # distribution thus makes this relative to te centre of the central cell.\n",
    "    #\n",
    "    # This value is not scaled relative to the size of the grid as we expect\n",
    "    # that the predictions should be scale invariant, and we do not want the\n",
    "    # network to unnecessarily learn the size of coarse_grid_sep.\n",
    "    x_offset = np.random.uniform(-0.5, 1.5)\n",
    "    t_offset = np.random.uniform(-0.5, 1.5)\n",
    "    \n",
    "    # Features: the solution on the coarse grid and the point to interpolate at.\n",
    "    X = solution.on_grid(cg, extra=2)\n",
    "    # We tell the network the offset; as the network has no way of knowing the\n",
    "    # location of the grid then adding a translation would only confuse it.\n",
    "    X[-2] = t_offset - 0.5  # -0.5 to normalise\n",
    "    X[-1] = x_offset - 0.5  # -0.5 to normalise\n",
    "    \n",
    "    t, x = point\n",
    "    # Label: the solution at the interpolation point\n",
    "    y = np.full(1, peakon((t + t_offset * coarse_grid_sep.t, \n",
    "                           x + x_offset * coarse_grid_sep.x)))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# A particularly nice X, y that is right on the peak of the peakon\n",
    "X_peak = np.array([0.71136994, 0.64367414, 0.58242045, 0.52699581, 0.47684553,\n",
    "                   0.43146768, 0.3904081 , 0.35325586, 1.53965685, 1.39313912,\n",
    "                   1.26056441, 1.14060584, 1.03206285, 0.93384908, 0.84498159,\n",
    "                   0.76457096, 3.33236346, 3.01524715, 2.72830845, 2.46867557,\n",
    "                   2.23375003, 2.02118061, 1.82883985, 1.65480272, 7.21241639,\n",
    "                   6.52606422, 5.9050271 , 5.34308947, 4.83462728, 4.37455167,\n",
    "                   3.95825804, 3.58157998, 3.81911647, 4.22077645, 4.66467938,\n",
    "                   5.155268  , 5.69745227, 6.29665855, 6.95888391, 7.69075612,\n",
    "                   1.76455206, 1.95013162, 2.15522875, 2.38189614, 2.63240234,\n",
    "                   2.90925451, 3.21522348, 3.55337148, 0.81527861, 0.90102221,\n",
    "                   0.99578354, 1.10051101, 1.21625277, 1.34416719, 1.48553448,\n",
    "                   1.64176951, 0.37668439, 0.41630063, 0.46008335, 0.50847074,\n",
    "                   0.56194707, 0.62104756, 0.68636371, 0.75854921])\n",
    "y_peak = np.array([5.34308947, 5.28992485, 5.23728921, 5.18517732, 5.13358394,\n",
    "                   5.08250393, 5.03193217, 4.98186361, 4.93229323, 4.8832161 ,\n",
    "                   4.83462728, 5.77198627, 5.71455405, 5.65769329, 5.6013983 ,\n",
    "                   5.54566345, 5.49048318, 5.43585196, 5.38176433, 5.32821488,\n",
    "                   5.27519826, 5.22270916, 6.23531118, 6.1732688 , 6.11184375,\n",
    "                   6.05102988, 5.99082113, 5.93121147, 5.87219493, 5.81376561,\n",
    "                   5.75591767, 5.69864534, 5.64194287, 6.73582778, 6.66880518,\n",
    "                   6.60244946, 6.53675399, 6.4717122 , 6.40731759, 6.34356371,\n",
    "                   6.2804442 , 6.21795273, 6.15608307, 6.09482902, 7.27652151,\n",
    "                   7.20411891, 7.13243673, 7.0614678 , 6.99120502, 6.92164137,\n",
    "                   6.85276989, 6.78458369, 6.71707595, 6.65023993, 6.58406894,\n",
    "                   7.58429925, 7.66052273, 7.70496678, 7.62830108, 7.55239822,\n",
    "                   7.4772506 , 7.40285071, 7.32919112, 7.25626445, 7.18406341,\n",
    "                   7.11258079, 7.0207356 , 7.09129517, 7.16256387, 7.23454883,\n",
    "                   7.30725726, 7.38069641, 7.45487364, 7.52979637, 7.60547208,\n",
    "                   7.68190835, 7.68351697, 6.49904846, 6.56436498, 6.63033795,\n",
    "                   6.69697395, 6.76427966, 6.8322618 , 6.90092717, 6.97028264,\n",
    "                   7.04033515, 7.11109169, 7.18255935, 6.01612613, 6.0765892 ,\n",
    "                   6.13765994, 6.19934444, 6.26164889, 6.32457951, 6.38814259,\n",
    "                   6.45234449, 6.51719163, 6.58269049, 6.64884763, 5.56908812,\n",
    "                   5.62505839, 5.68159116, 5.7386921 , 5.79636692, 5.85462137,\n",
    "                   5.9134613 , 5.97289257, 6.03292114, 6.093553  , 6.15479423,\n",
    "                   5.155268  , 5.2070793 , 5.25941132, 5.31226928, 5.36565848,\n",
    "                   5.41958424, 5.47405197, 5.5290671 , 5.58463515, 5.64076167,\n",
    "                   5.69745227])\n",
    "\n",
    "\n",
    "class BatchData:\n",
    "    \"\"\"Wrapper around a function that generates a single data point, and produces\n",
    "    batches in the manner expected by TensorFlow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gen_one_data, batch_size, batch_reuse=1):\n",
    "        \"\"\"The argument :gen_one_data: is the function to generate a single data\n",
    "        point. It will be called without arguments. The :batch_size: may also be\n",
    "        set; it defaults to 10.\n",
    "        \n",
    "        There is also a :batch_reuse: argument. If generating data is intensive,\n",
    "        then we may wish to use it several times to train our network before\n",
    "        discarding it. This makes sense because we're working in a somewhat\n",
    "        different context to that of normal machine learning, as we can generate\n",
    "        an infinite amount of data (as that's just mathematical computations).\n",
    "        The idea is roughly equivalent to reusing the same data between epochs.\n",
    "        \n",
    "        Calls to BatchData for batches of data will return the same batch of\n",
    "        data :batch_reuse: times before generating a new batch of data.\n",
    "        \n",
    "        The :batch_reuse: argument may also be set to None to have BatchData\n",
    "        endlessly give exactly the same batch of data. For example when\n",
    "        comparing different neural networks: giving each network the same data\n",
    "        in evaluation ensures a fair test.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gen_one_data = gen_one_data\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_reuse = batch_reuse\n",
    "        \n",
    "        # How many times this batch of data has been served so far\n",
    "        self._batch_reuse_index = 0\n",
    "        \n",
    "        # Will be assigned values in self._gen_data()\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self._gen_data()\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Generates a batch of data.\"\"\"\n",
    "        \n",
    "        if self.batch_reuse is not None:\n",
    "            if self._batch_reuse_index >= self.batch_reuse:\n",
    "                self._gen_data()\n",
    "                self._batch_reuse_index = 1\n",
    "            else:\n",
    "                self._batch_reuse_index += 1\n",
    "            \n",
    "        return self.X, self.y\n",
    "    \n",
    "    def _gen_data(self):\n",
    "        \"\"\"Generates self.batch_size amount of data.\"\"\"\n",
    "        \n",
    "        X_new, y_new = self.gen_one_data()\n",
    "        \n",
    "        X = np.empty((self.batch_size, *X_new.shape), dtype=X_new.dtype)\n",
    "        y = np.empty((self.batch_size, *y_new.shape), dtype=y_new.dtype)\n",
    "        \n",
    "        if self.batch_size >= 0:\n",
    "            X[0] = X_new\n",
    "            y[0] = y_new\n",
    "            for i in range(1, self.batch_size):\n",
    "                X_new, y_new = self.gen_one_data()\n",
    "                X[i] = X_new\n",
    "                y[i] = y_new\n",
    "                \n",
    "        # For some mad reason TensorFlow seems to demand that the features be \n",
    "        # wrapped in a dictionary but that the labels are not.\n",
    "        self.X = {'X': X}\n",
    "        self.y = y\n",
    "        \n",
    "    @classmethod\n",
    "    def from_single_data(cls, X, y):\n",
    "        return cls(gen_one_data=lambda: (X, y), batch_size=1)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_data(cls, X, y):\n",
    "        assert len(X) == len(y)\n",
    "        \n",
    "        i = -1\n",
    "        def gen_one_data():\n",
    "            nonlocal i  # Yuck, but I think this is the cleanest way\n",
    "            i += 1\n",
    "            return X[i], y[i]\n",
    "        \n",
    "        return cls(gen_one_data, batch_size=len(X))\n",
    "    \n",
    "    @classmethod\n",
    "    def test(cls, gen_one_data, batch_size=1):\n",
    "        \"\"\"Generates :batch_size: amount of test data, where :gen_one_data: is a \n",
    "        function for generating one piece of data.\"\"\"\n",
    "        return cls(gen_one_data, batch_size, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preprocessing\n",
    "\n",
    "# Not using sklearn's Pipelines as they don't provide good ways of processing the\n",
    "# labels.\n",
    "class Processor(tools.SubclassTrackerMixin('__name__')):\n",
    "    \"\"\"Base class for preprocessors.\"\"\"\n",
    "    \n",
    "    save_attr = []\n",
    "    init_attr = []\n",
    "    checkpoint_filename = 'processor-checkpoint'\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.batch_data = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self._training = True\n",
    "        super(Processor, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def data(self, batch_data):\n",
    "        self.batch_data = batch_data\n",
    "        return self  # for chaining\n",
    "    \n",
    "    def training(self, val):\n",
    "        self._training = val\n",
    "        return self  # for chaining\n",
    "        \n",
    "    def __call__(self, use_tf=True):\n",
    "        \"\"\"If :use_tf: is True, which it defaults to, then outputted data is \n",
    "        wrapped in tf.constant, which is the form TensorFlow is expecting it in.\n",
    "        The :use_tf: argument may be set to False to instead output the raw data,\n",
    "        if the regressor is not using TensorFlow.\"\"\"\n",
    "        def wrapped_call():\n",
    "            if self.batch_data is None:\n",
    "                raise RuntimeError(\"Must be passed data via 'init' method before \"\n",
    "                                   \"being called.\")\n",
    "                \n",
    "            X, y = self.batch_data()\n",
    "            X = X['X']\n",
    "            X, y = self.transform(X, y)\n",
    "            X = {'X': X}\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "            \n",
    "            if use_tf:\n",
    "                tf_x = {key: tf.constant(val) for key, val in self.X.items()}\n",
    "                return tf_x, tf.constant(self.y)\n",
    "            else:\n",
    "                return X, y\n",
    "        return wrapped_call\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        \"\"\"Processes the data.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def inverse_transform(self, y):\n",
    "        \"\"\"Performs the inverse transform on the data.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def save(self, model_dir, step):\n",
    "        write_dict = {'save': {name: getattr(self, name) for name in self.save_attr},\n",
    "                      'init': {name: getattr(self, name) for name in self.init_attr}}\n",
    "        with open(model_dir + '/' + self.checkpoint_filename, 'w') as f:\n",
    "            f.write(json.dumps(write_dict))\n",
    "        tflog.info('Saving processor checkpoint for {} into {}'.format(step, model_dir))\n",
    "        \n",
    "    @classmethod\n",
    "    def _get_load_dict(cls, model_dir):\n",
    "        with open(model_dir + '/' + cls.checkpoint_filename, 'r') as f:\n",
    "            return json.loads(f.read())\n",
    "        \n",
    "    def _load(self, load_dict):\n",
    "        tflog.info(\"Processor checkpoint file found, restoring values.\")\n",
    "        training_val = self._training\n",
    "        self._training = True  # to make sure that we can set values\n",
    "        for key, val in load_dict['save'].items():\n",
    "            setattr(self, key, val)\n",
    "        self._training = training_val\n",
    "    \n",
    "    def load(self, model_dir):\n",
    "        try:\n",
    "            load_dict = self._get_load_dict(model_dir)\n",
    "        except FileNotFoundError:\n",
    "            tflog.info(\"No processor checkpoint file '{}' found.\"\n",
    "                       .format(model_dir + '/' + self.checkpoint_filename))\n",
    "        else:\n",
    "            self._load(load_dict)\n",
    "            \n",
    "    @classmethod\n",
    "    def load_init(cls, model_dir):\n",
    "        load_dict = cls._get_load_dict(model_dir)\n",
    "        self = cls(**load_dict['init'])\n",
    "        self._load(load_dict)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "class IdentityProcess(Processor):\n",
    "    \"\"\"Performs no processing, but still uses Processor so that the conversion\n",
    "    to TensorFlow (via the use_tf argument for __call__) may still be performed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        return X, y\n",
    "    \n",
    "    def inverse_transform(self, y):\n",
    "        return y\n",
    "  \n",
    "    \n",
    "class ScaleDataOverall(Processor):\n",
    "    \"\"\"Scales data to between -1 and 1. Scaling is done across all batches.\"\"\"\n",
    "    \n",
    "    init_attr = ['momentum']\n",
    "    save_attr = ['X_mean', 'X_extent']\n",
    "    \n",
    "    def __init__(self, momentum=0.99, **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self._X_mean = None\n",
    "        self._X_extent = None\n",
    "        super(ScaleDataOverall, self).__init__(**kwargs)\n",
    "        \n",
    "    @property\n",
    "    def X_mean(self):\n",
    "        return self._X_mean\n",
    "    \n",
    "    @X_mean.setter\n",
    "    def X_mean(self, value):\n",
    "        if self._training:\n",
    "            self._X_mean = value\n",
    "        \n",
    "    @property\n",
    "    def X_extent(self):\n",
    "        return self._X_extent\n",
    "    \n",
    "    @X_extent.setter\n",
    "    def X_extent(self, value):\n",
    "        if self._training:\n",
    "            self._X_extent = value\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        X_mean = np.mean(X)\n",
    "        X_extent = np.max(np.abs(X - X_mean))\n",
    "        try:\n",
    "            self.X_mean = self.X_mean * self.momentum + X_mean * (1 - self.momentum)\n",
    "            self.X_extent = self.X_extent * self.momentum + X_extent * (1 - self.momentum)\n",
    "        except TypeError:\n",
    "            # First time around so self.X_mean is None\n",
    "            self.X_mean = X_mean\n",
    "            self.X_extent = X_extent\n",
    "        \n",
    "        X_scaled = (X - self.X_mean) / self.X_extent\n",
    "        y_scaled = (y - self.X_mean) / self.X_extent\n",
    "        return X_scaled, y_scaled\n",
    "    \n",
    "    def inverse_transform(self, y):\n",
    "        return (y * self.X_extent) + self.X_mean\n",
    "    \n",
    "    \n",
    "class NormalisationOverall(Processor):\n",
    "    \"\"\"Normalises inputs by subtracting mean and dividing by standard deviation.\n",
    "    Scaling is done across all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    init_attr = ['momentum']\n",
    "    save_attr = ['mean', 'stddev']\n",
    "    \n",
    "    def __init__(self, momentum=0.99, **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self._mean = None\n",
    "        self._stddev = None\n",
    "        super(NormalisationOverall, self).__init__(**kwargs)\n",
    "        \n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self._mean\n",
    "    \n",
    "    @mean.setter\n",
    "    def mean(self, value):\n",
    "        if self._training:\n",
    "            self._mean = value\n",
    "        \n",
    "    @property\n",
    "    def stddev(self):\n",
    "        return self._stddev\n",
    "    \n",
    "    @stddev.setter\n",
    "    def stddev(self, value):\n",
    "        if self._training:\n",
    "            self._stddev = value\n",
    "            \n",
    "    def transform(self, X, y):\n",
    "        mean = np.mean(X)\n",
    "        stddev = np.sqrt(np.mean(np.square(X - mean)))\n",
    "        try:\n",
    "            self.mean = self.mean * self.momentum + mean * (1 - self.momentum)\n",
    "            self.stddev = self.stddev * self.momentum + stddev * (1 - self.momentum)\n",
    "        except TypeError:\n",
    "            # First time around so self.X_mean is None\n",
    "            self.mean = mean\n",
    "            self.stddev = stddev\n",
    "            \n",
    "        X_scaled = (X - self.mean) / self.stddev\n",
    "        y_scaled = (y - self.mean) / self.stddev\n",
    "        \n",
    "        return X_scaled, y_scaled\n",
    "    \n",
    "    def inverse_transform(self, y):\n",
    "        return (y * self.stddev) + self.mean\n",
    "    \n",
    "    \n",
    "### Processor Saving Hooks\n",
    "\n",
    "class ProcessorSavingHook(tft.SessionRunHook):\n",
    "    \"\"\"Saves the processor data.\"\"\"\n",
    "    \n",
    "    def __init__(self, processor, model_dir, save_secs=600, \n",
    "                 save_steps=None, **kwargs):\n",
    "        self.processor = processor\n",
    "        self.model_dir = model_dir\n",
    "        self._timer = tft.SecondOrStepTimer(every_secs=save_secs,\n",
    "                                            every_steps=save_steps)\n",
    "        self._global_step_tensor = None\n",
    "        super(ProcessorSavingHook, self).__init__(**kwargs)\n",
    "    \n",
    "    def begin(self):\n",
    "        self._global_step_tensor = tft.get_global_step()\n",
    "        \n",
    "    def after_create_session(self, session, coord):\n",
    "        global_step = session.run(self._global_step_tensor)\n",
    "        self._save(session, global_step)\n",
    "        self._timer.update_last_triggered_step(global_step)\n",
    "        \n",
    "    def before_run(self, run_context):\n",
    "        return tft.SessionRunArgs(self._global_step_tensor)\n",
    "        \n",
    "    def after_run(self, run_context, run_values):\n",
    "        stale_global_step = run_values.results\n",
    "        if self._timer.should_trigger_for_step(stale_global_step + 1):\n",
    "            global_step = run_context.session.run(self._global_step_tensor)\n",
    "            if self._timer.should_trigger_for_step(global_step):\n",
    "                self._timer.update_last_triggered_step(global_step)\n",
    "                self._save(run_context.session, global_step)\n",
    "            \n",
    "    def end(self, session):\n",
    "        last_step = session.run(self._global_step_tensor)\n",
    "        if last_step != self._timer.last_triggered_step():\n",
    "            self._save(session, last_step)\n",
    "        \n",
    "    def _save(self, session, step):\n",
    "        self.processor.save(self.model_dir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DNN Construction\n",
    "\n",
    "# Keras-inspired nice interface, just without the slow speed and lack of \n",
    "# multicore functionality of Keras...\n",
    "\n",
    "# Lambda wrappers so that each call to model_fn in Sequential.compile creates a\n",
    "# new TensorFlow graph and thus allows repeated training without redefining the\n",
    "# Estimator\n",
    "def dense(*args, **kwargs):\n",
    "    \"\"\"Lambda wrapper around tf.layers.Dense.\"\"\"\n",
    "    return lambda: tfla.Dense(*args, **kwargs)\n",
    "\n",
    "\n",
    "def dropout(*args, **kwargs):\n",
    "    \"\"\"Lambda wrapper around tf.layers.Dropout.\"\"\"\n",
    "    return lambda: tfla.Dropout(*args, **kwargs)\n",
    "\n",
    "# Not currently working... :(\n",
    "def alpha_dropout(*args, **kwargs):\n",
    "    \"\"\"Lambda wrapper around tf.keras.layers.AlphaDropout.\"\"\"\n",
    "    return lambda: tfk.layers.AlphaDropout(*args, **kwargs, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def batch_normalization(*args, **kwargs):\n",
    "    \"\"\"Lambda wrapper around tf.layers.BatchNormalization.\"\"\"\n",
    "    return lambda: tfla.BatchNormalization(*args, **kwargs)\n",
    "\n",
    "\n",
    "class Sequential:\n",
    "    \"\"\"Defines a neural network. Expected usage is roughly:\n",
    "    \n",
    "    >>> model = Sequential()\n",
    "    >>> model.add(dense(units=100, activation=tf.nn.relu))\n",
    "    >>> model.add_train(dropout(rate=0.4))\n",
    "    >>> model.add(dense(units=50, activation=tf.nn.relu))\n",
    "    >>> model.add_train(dropout(rate=0.4))\n",
    "    >>> model.add(dense(units=10, activation=tf.nn.relu))\n",
    "    \n",
    "    to define the neural network in the abstract (note that the last dense layer\n",
    "    are treated as the logits), followed by:\n",
    "    \n",
    "    >>> dnn = model.compile()\n",
    "    \n",
    "    to actually create it in TensorFlow. Here, 'dnn' is a tf.Estimator, so may\n",
    "    be used like:\n",
    "    \n",
    "    >>> dnn.train(...)\n",
    "    >>> dnn.predict(...)\n",
    "    >>> dnn.evaluate(...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Creates a Sequential. See Sequential.__doc__ for more info.\"\"\"\n",
    "        self._layer_funcs = []\n",
    "        self._layer_train = []\n",
    "        self._kwargs = {}\n",
    "        \n",
    "    def add(self, layer):\n",
    "        \"\"\"Add a layer to the network. Should be the result of dense, above.\n",
    "        \"\"\"\n",
    "        self._layer_funcs.append(layer)\n",
    "        self._layer_train.append(False)\n",
    "        \n",
    "    def add_train(self, layer):\n",
    "        \"\"\"Add a layer to the network which needs to know if the network is in\n",
    "        training or not. Should be the result of dropout, alpha_dropout or \n",
    "        batch_normalization, above.\n",
    "        \"\"\"\n",
    "        self.add(layer)\n",
    "        self._layer_train[-1] = True\n",
    "        \n",
    "    def set_kwargs(self, **kwargs):\n",
    "        \"\"\"Sets the custom keyword arguments to be passed to the estimators\n",
    "        created during compile time.\n",
    "        \"\"\"\n",
    "        self._kwargs = kwargs\n",
    "        \n",
    "    def update_kwargs(self, **kwargs):\n",
    "        \"\"\"Updates the custom keyword arguments to be passed to the estimators\n",
    "        created during compile time.\n",
    "        \"\"\"\n",
    "        self._kwargs.update(kwargs)\n",
    "        \n",
    "    def reset_kwargs(self):\n",
    "        \"\"\"Resets the custom keyword arguments to be passed to the estimators\n",
    "        created during compile time.\n",
    "        \"\"\"\n",
    "        self._kwargs = {}\n",
    "        \n",
    "    def compile(self, optimizer=None, loss_fn=tflo.mean_squared_error, \n",
    "                gradient_clip=None, **kwargs):\n",
    "        \"\"\"Takes its abstract neural network definition and compiles it into a\n",
    "        tf.estimator.Estimator.\n",
    "        \n",
    "        May be given an :optimizer:, defaulting to tf.train.AdamOptimizer().\n",
    "        May be given a :loss_fn:, defaulting to tf.losses.mean_squared_error.\n",
    "        May be given a :gradient_clip:, defaulting to no clipping.\n",
    "        \n",
    "        Any additional kwargs are passed into the creation of the\n",
    "        tf.estimator.Estimator.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Probably shouldn't use the same optimizer instance every time? Hence\n",
    "        # this.\n",
    "        if optimizer is None:\n",
    "            optimizer = tft.AdamOptimizer()\n",
    "            \n",
    "        def model_fn(features, labels, mode):\n",
    "            # First layer is the feature inputs.\n",
    "            layers = [features[\"X\"]]\n",
    "            \n",
    "            for prev_layer, layer_func, train in zip(layers, self._layer_funcs, \n",
    "                                                     self._layer_train):\n",
    "                if train:\n",
    "                    layer = layer_func()(inputs=prev_layer, \n",
    "                                         training=mode == tfe.ModeKeys.TRAIN)\n",
    "                else:\n",
    "                    layer = layer_func()(inputs=prev_layer)\n",
    "                    \n",
    "                # Deliberately using the generator nature of zip to add elements\n",
    "                # to the layers list as we're iterating through it.\n",
    "                # https://media.giphy.com/media/3oz8xtBx06mcZWoNJm/giphy.gif\n",
    "                layers.append(layer)\n",
    "                \n",
    "            logits = layers[-1]\n",
    "            \n",
    "            if mode == tfe.ModeKeys.PREDICT:\n",
    "                return tfe.EstimatorSpec(mode=mode, predictions=logits)\n",
    "            \n",
    "            loss = loss_fn(labels, logits)\n",
    "\n",
    "            if mode == tfe.ModeKeys.TRAIN:\n",
    "                g_step = tft.get_global_step()\n",
    "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "                with tf.control_dependencies(update_ops):\n",
    "                    # Gradient clipping\n",
    "                    if gradient_clip is None:\n",
    "                        train_op = optimizer.minimize(loss=loss, global_step=g_step)\n",
    "                    else:\n",
    "                        gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "    #                     gradients0 = tf.Print(gradients[0], [tf.global_norm(gradients)], 'Global norm: ')\n",
    "    #                     gradients = tuple([gradients0, *gradients[1:]])\n",
    "                        gradients, _ = tf.clip_by_global_norm(gradients, \n",
    "                                                              gradient_clip)\n",
    "                        train_op = optimizer.apply_gradients(zip(gradients, \n",
    "                                                                 variables),\n",
    "                                                             global_step=g_step)\n",
    "                return tfe.EstimatorSpec(mode=mode, loss=loss, \n",
    "                                         train_op=train_op)\n",
    "            \n",
    "            if mode == tfe.ModeKeys.EVAL:\n",
    "                return tfe.EstimatorSpec(mode=mode, loss=loss)\n",
    "            \n",
    "            raise RuntimeError(\"mode '{}' not understood\".format(mode))\n",
    "                \n",
    "        tools.update_without_overwrite(kwargs, self._kwargs)\n",
    "        return tfe.Estimator(model_fn=model_fn, **kwargs)\n",
    "    \n",
    "    \n",
    "def model_dir_str(model_dir, hidden_units, logits, drop_rate, drop_type, \n",
    "                  preprocessor, activation, uuid=None):\n",
    "    \"\"\"Returns a string for the model directory describing the network.\"\"\"\n",
    "    \n",
    "    layer_counter = [(k, sum(1 for _ in g)) for k, g in it.groupby(hidden_units)]\n",
    "    for layer_size, layer_repeat in layer_counter:\n",
    "        if layer_repeat == 1:\n",
    "            model_dir += '{}_'.format(layer_size)\n",
    "        else:\n",
    "            model_dir += '{}x{}_'.format(layer_size, layer_repeat)\n",
    "    model_dir += '{}_'.format(logits)\n",
    "    if drop_type in ('dropout', 'normal'):\n",
    "        model_dir += 'D0'\n",
    "    elif drop_type in ('alpha', 'alpha_dropout'):\n",
    "        model_dir += 'A0'\n",
    "    else:\n",
    "        raise RuntimeError(\"Unrecognised drop_type '{}'\".format(drop_type))\n",
    "    model_dir += '{}_'.format(int(drop_rate * 10))\n",
    "    model_dir += preprocessor.__class__.__name__\n",
    "    \n",
    "    if isinstance(activation, ft.partial):\n",
    "        activation_fn = activation.func\n",
    "        alpha = str(activation.keywords['alpha']).replace('.', '')\n",
    "    else:\n",
    "        activation_fn = activation\n",
    "        alpha = '02'\n",
    "        \n",
    "    model_dir += '_' + activation_fn.__name__.replace('_', '')\n",
    "    if activation_fn is tf.nn.leaky_relu:\n",
    "        model_dir += alpha\n",
    "\n",
    "    if uuid not in (None, ''):\n",
    "        model_dir += '_' + str(uuid)\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "def dnn_hyperparameters_from_dir(dir_name):\n",
    "    \"\"\"Creates DNN hyperparameters from the name of the directory of the DNN.\n",
    "    \"\"\"\n",
    "    \n",
    "    dnn_details = {}\n",
    "\n",
    "    if 'D0' in dir_name:\n",
    "        units, rest = dir_name.split('D0')\n",
    "        dnn_details['drop_type'] = 'dropout'\n",
    "    elif 'A0' in dir_name:\n",
    "        units, rest = dir_name.split('A0')\n",
    "        dnn_details['drop_type'] = 'alpha_dropout'\n",
    "    else:\n",
    "        raise RuntimeError(\"Bad dir_name string '{}'. Cannot split on 'D0' \"\n",
    "                           \"or 'A0'.\".format(dir_name))\n",
    "    units = units.split('_')[:-1]  # last element is ''\n",
    "    rest = rest.split('_')\n",
    "    \n",
    "    all_units = []\n",
    "    for unit in units:\n",
    "        if 'x' in unit:\n",
    "            unit_size, unit_repeat = unit.split('x')\n",
    "            unit_size, unit_repeat = int(unit_size), int(unit_repeat)\n",
    "            all_units.extend([unit_size for _ in range(unit_repeat)])\n",
    "        else:\n",
    "            all_units.append(int(unit))\n",
    "    dnn_details['hidden_units'] = all_units[:-1]\n",
    "    dnn_details['logits'] = all_units[-1]\n",
    "    \n",
    "    dnn_details['drop_rate'] = int(rest[0])\n",
    "    preprocessor_name = rest[1]\n",
    "    preprocessor_class = Processor.find_subclass(preprocessor_name)\n",
    "    dnn_details['batch_norm'] = False\n",
    "    \n",
    "    activation_name = rest[2].lower()\n",
    "    \n",
    "    # Not a great way to do this inversion, admittedly\n",
    "    if activation_name == 'elu':\n",
    "        dnn_details['activation'] = tf.nn.elu\n",
    "    elif activation_name == 'relu':\n",
    "        dnn_details['activation'] = tf.nn.relu\n",
    "    elif activation_name == 'selu':\n",
    "        dnn_details['activation'] = tf.nn.selu\n",
    "    elif activation_name[:9] == 'leakyrelu':\n",
    "        alpha = float(str(activation_name[9]) + '.' + str(activation_name[10:]))\n",
    "        dnn_details['activation'] = ft.partial(tf.nn.leaky_relu, alpha=alpha)\n",
    "    else:\n",
    "        raise RuntimeError(\"Activation '{}' not understood.\".format(activation_name))\n",
    "        \n",
    "    remaining = rest[3:]\n",
    "    if len(remaining) == 0:\n",
    "        uuid = None\n",
    "    elif len(remaining) == 1:\n",
    "        uuid = remaining[0]\n",
    "    else:\n",
    "        raise RuntimeError(\"Bad dir_name string '{}'. Too many remaining \"\n",
    "                           \"arguments: {}\".format(dir_name, remaining))\n",
    "        \n",
    "    return dnn_details, preprocessor_class, uuid\n",
    "    \n",
    "    \n",
    "def create_dnn(hidden_units, logits, activation=tf.nn.relu, \n",
    "               drop_rate=0.0, drop_type='dropout', model_dir=None, log_steps=100, \n",
    "               gradient_clip=None, batch_norm=False,\n",
    "               kernel_initializer=tfi.truncated_normal(mean=0, stddev=0.05)\n",
    "               ):\n",
    "    \"\"\"Shortcut for creating a simple DNN with dense, dropout and batch \n",
    "    normalization layers, and then compiling it.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    if batch_norm:\n",
    "        model.add_train(batch_normalization())\n",
    "    for units in hidden_units:\n",
    "        model.add(dense(units=units, activation=activation,\n",
    "                        kernel_initializer=kernel_initializer))\n",
    "        if batch_norm:\n",
    "            model.add_train(batch_normalization())\n",
    "        if drop_rate != 0:\n",
    "            if drop_type == 'dropout':\n",
    "                model.add_train(dropout(rate=drop_rate))\n",
    "            elif drop_type in ('alpha', 'alpha_dropout'):\n",
    "                model.add_train(alpha_dropout(rate=drop_rate))\n",
    "    model.add(dense(units=logits))\n",
    "    model.set_kwargs(model_dir=model_dir,\n",
    "                     config=tfe.RunConfig(log_step_count_steps=log_steps))\n",
    "\n",
    "    return model.compile(gradient_clip=gradient_clip)\n",
    "\n",
    "\n",
    "def create_dnn_pro_from_model_dir(model_dir, log_steps=100, gradient_clip=None):\n",
    "    \"\"\"Creates a DNN and processor from their model_dir.\"\"\"\n",
    "    if model_dir[-1] in ('/', '\\\\'):\n",
    "        model_dir = model_dir[:-1]\n",
    "    model_dir_split = tools.split(['/', '\\\\'], model_dir)\n",
    "    dir_name = model_dir_split[-1]\n",
    "    dnn_details, preprocessor_class, uuid = dnn_hyperparameters_from_dir(dir_name)\n",
    "    preprocessor = preprocessor_class.load_init(model_dir)\n",
    "    dnn = create_dnn(**dnn_details)\n",
    "    return dnn, preprocessor, uuid\n",
    "\n",
    "\n",
    "def create_dnns_pros_from_dir(dir_, log_steps=100, gradient_clip=None, \n",
    "                              exclude_start=('.',), exclude_end=(),\n",
    "                              exclude_in=()):\n",
    "    \"\"\"Creates multiple DNNs and processors from a directory containing the\n",
    "    directories for multiple DNNs and processors.\n",
    "    \"\"\"\n",
    "    \n",
    "    subdirectories = sorted(next(os.walk(dir_))[1])\n",
    "    if dir_[-1] in ('/', '\\\\'):\n",
    "        dir_ = dir_[:-1]\n",
    "    dnns_processors = []\n",
    "    \n",
    "    for subdir in subdirectories:\n",
    "        if any(subdir.startswith(ex) for ex in exclude_start):\n",
    "            tflog.warn(\"Excluding '{}' based on start.\".format(subdir))\n",
    "            continue\n",
    "        if any(subdir.endswith(ex) for ex in exclude_end):\n",
    "            tflog.warn(\"Excluding '{}' based on end.\".format(subdir))\n",
    "            continue\n",
    "        if any(ex in subdir for ex in exclude_in):\n",
    "            tflog.warn(\"Excluding '{}' based on containment.\".format(subdir))\n",
    "            continue\n",
    "            \n",
    "        model_dir = dir_ + '/' + subdir\n",
    "        try:\n",
    "            dnn, processor, uuid = create_dnn_pro_from_model_dir(model_dir,\n",
    "                                                                 log_steps=log_steps,\n",
    "                                                                 gradient_clip=gradient_clip)\n",
    "        except (FileNotFoundError, RuntimeError) as e:\n",
    "            tflog.warn(\"Could not load DNN from '{}'. Error message: '{}'\"\n",
    "                       .format(subdir, e))\n",
    "        else:\n",
    "            dnns_processors.append((dnn, processor))\n",
    "            \n",
    "    return dnns_processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simpler interpolation methods\n",
    "# Useful to give a baseline to compare the neural network models against.\n",
    "\n",
    "class _RegressorBase:\n",
    "    \"\"\"Base class for performing predictions based on just the input. Subclasses\n",
    "    are expected to provide a predict_single classmethod specifying their\n",
    "    predictions.\n",
    "    \n",
    "    Its predict and evaluate methods are designed to resemble that of\n",
    "    tf.estimator.Estimator's, so that we can call them in the same way. (We don't\n",
    "    actually inherit from tf.estimator.Estimator because none of what these \n",
    "    classes do uses TensorFlow, so messing around with model functions and\n",
    "    EstimatorSpecs is just unnecessary faff and overhead.)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flag to tell test_regressor (and thus BatchData) that this regressor\n",
    "    # doesn't use TensorFlow.\n",
    "    use_tf = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _index_tol(cg, point, tol=0.001):\n",
    "        \"\"\"Searches through a list of 2-tuples, :cg:, to find the first element \n",
    "        which is within tolerance :tol: of :point:. Essentially the index method\n",
    "        for lists, except this one makes sense for high precision floating point\n",
    "        numbers.\n",
    "        \"\"\"\n",
    "        \n",
    "        t, x = point\n",
    "        for i, element in enumerate(cg):\n",
    "            t2, x2 = element\n",
    "            if max(np.abs(t - t2), np.abs(x - x2)) < tol:\n",
    "                return i\n",
    "        raise ValueError('{} is not in {}'.format(point, type(cg)))\n",
    "        \n",
    "    def _prepare(self, Xi):\n",
    "        \"\"\"Performs any necessary preparations on the data :Xi: before making \n",
    "        predictions.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _interp(self, Xi, point):\n",
    "        \"\"\"Helper function for performing interpolation on a coarse\n",
    "        grid :Xi:, giving the value of the interpolation at :point:.\n",
    "        \n",
    "        The spacing of the grid is known from the global hyperparameters\n",
    "        defining the coarse grid size, whilst it isn't necessary to know its\n",
    "        location.\n",
    "        \n",
    "        The argument :point: should be scaled to the grid size, i.e.\n",
    "        coarse_grid_sep.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict_single(self, Xi, y):\n",
    "        \"\"\"Makes a prediction corresponding to input feature :Xi:.\n",
    "        \n",
    "        It is given the true result :y:. Not to cheat and return perfect\n",
    "        results, but to determine its shape etc.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict(self, input_fn, yield_single_examples=False):\n",
    "        \"\"\"The argument :input_fn: should probably be an instance of BatchData.\n",
    "        \n",
    "        The argument :yield_single_examples: is there for compatibility with the\n",
    "        interface for the usual TF Estimators and is ignored.\n",
    "        \"\"\"\n",
    "        \n",
    "        returnval = []\n",
    "        X, y = input_fn()\n",
    "        \n",
    "        for Xi in X['X']:\n",
    "            returnval.append(self.predict_single(Xi, y))\n",
    "            \n",
    "        returnval = np.array(returnval)\n",
    "        while True:\n",
    "            yield returnval\n",
    "            \n",
    "    def evaluate(self, input_fn, steps=None):\n",
    "        \"\"\"The argument :input_fn: should probably be an instance of BatchData.\n",
    "        \n",
    "        The number of steps is given by :steps:, with None corresponding to\n",
    "        infinity. The evaluation will terminate earlier if input_fn raises a\n",
    "        StopIteration or a tf.errors.OutOfRangeError.\"\"\"\n",
    "        \n",
    "        if steps is None:\n",
    "            steps = np.inf\n",
    "        \n",
    "        losses = []\n",
    "        for step in tools.rangeinf(0, steps):\n",
    "            try:\n",
    "                X, y = input_fn()\n",
    "            except (StopIteration, tfer.OutOfRangeError):\n",
    "                break\n",
    "            predictor = self.predict(lambda: (X, y))\n",
    "            prediction = next(predictor)\n",
    "            losses.append(np.mean(np.square(y - prediction)))\n",
    "            \n",
    "        return {'loss': np.mean(losses), 'global_step': None}\n",
    "\n",
    "\n",
    "class BilinearInterpBase(_RegressorBase):\n",
    "    \"\"\"Base class for performing bilinear interpolation.\"\"\"\n",
    "        \n",
    "    def _interp(self, Xi, point):        \n",
    "        # The actual t, x values for the grid don't matter from this point \n",
    "        # onwards; so this is just a translation from wherever X was actually \n",
    "        # calculated. So WLOG assume it was around 0.\n",
    "        cg = coarse_grid((0, 0))\n",
    "        t, x = point\n",
    "        \n",
    "        # The grid points nearest :point:.\n",
    "        t_below = tools.round_mult(t, coarse_grid_sep.t, 'down')\n",
    "        t_above = tools.round_mult(t, coarse_grid_sep.t, 'up')\n",
    "        x_below = tools.round_mult(x, coarse_grid_sep.x, 'down')\n",
    "        x_above = tools.round_mult(x, coarse_grid_sep.x, 'up')\n",
    "        \n",
    "        # The value of :Xi: at those grid points.\n",
    "        t_b_x_b = Xi[self._index_tol(cg, (t_below, x_below))]\n",
    "        t_a_x_b = Xi[self._index_tol(cg, (t_above, x_below))]\n",
    "        t_b_x_a = Xi[self._index_tol(cg, (t_below, x_above))]\n",
    "        t_a_x_a = Xi[self._index_tol(cg, (t_above, x_above))]\n",
    "        \n",
    "        # Shift the t, x values to be relative to the bottom-left point of the\n",
    "        # grid square in which (t, x) lies.\n",
    "        t_scale = (t % coarse_grid_sep.t) / coarse_grid_sep.t\n",
    "        x_scale = (x % coarse_grid_sep.x) / coarse_grid_sep.x\n",
    "        \n",
    "        # Bilinear interpolation\n",
    "        returnval = (1 - t_scale) * (1 - x_scale) * t_b_x_b\n",
    "        returnval += t_scale * (1 - x_scale) * t_a_x_b\n",
    "        returnval += (1 - t_scale) * x_scale * t_b_x_a\n",
    "        returnval += t_scale * x_scale * t_a_x_a\n",
    "        \n",
    "        return returnval\n",
    "    \n",
    "    \n",
    "class PolyInterpBase(_RegressorBase):\n",
    "    \"\"\"Base class for performing polynomial interpolation.\"\"\"\n",
    "    \n",
    "    def __init__(self, poly_deg, *args, **kwargs):\n",
    "        self.poly_deg = poly_deg\n",
    "        self._poly_coefs = None\n",
    "        super(PolyInterpBase, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def poly(self, point):\n",
    "        \"\"\"Interprets its currently stored polynomial coefficients as a \n",
    "        polynomial, and evaluates them at the specified point.\"\"\"\n",
    "        \n",
    "        if self._poly_coefs is None:\n",
    "            raise RuntimeError('Must run _prepare first!')\n",
    "        \n",
    "        t, x = point\n",
    "        coefs = iter(self._poly_coefs)\n",
    "\n",
    "        result = next(coefs)  # Intercept, i.e. constant term\n",
    "        for power in range(1, self.poly_deg + 1):\n",
    "            for x_power in range(0, power + 1):\n",
    "                t_power = power - x_power\n",
    "                coef = next(coefs)\n",
    "                result += coef * (t ** t_power) * (x ** x_power)\n",
    "        try:\n",
    "            next_coef = next(coefs)\n",
    "        except StopIteration:\n",
    "            return result\n",
    "        else:\n",
    "            raise RuntimeError('coef_: {coef_}, poly_deg: {poly_deg}, '\n",
    "                               'coef that shouldn\\'t exist: {next_coef}'\n",
    "                               .format(coef_=coef_, \n",
    "                                       poly_deg=self.poly_deg, \n",
    "                                       next_coef=next_coef))\n",
    "    \n",
    "    def _prepare(self, Xi):\n",
    "        poly_features = skpr.PolynomialFeatures(degree=self.poly_deg, \n",
    "                                                include_bias=True)\n",
    "        lin_reg = sklm.LinearRegression(fit_intercept=False)\n",
    "        poly_pipe = skpi.Pipeline([('pf', poly_features), ('lr', lin_reg)])\n",
    "        \n",
    "        # The actual t, x values for the grid don't matter from this point \n",
    "        # onwards; so this is just a translation from wherever X was actually \n",
    "        # calculated. So WLOG assume it was around 0.\n",
    "        cg = coarse_grid((0, 0))\n",
    "        poly_pipe.fit(cg, Xi)\n",
    "        self._poly_coefs = poly_pipe.named_steps['lr'].coef_\n",
    "        \n",
    "    \n",
    "    def _interp(self, Xi, point):\n",
    "        return self.poly(point)\n",
    "    \n",
    "    \n",
    "class NearestInterpBase(_RegressorBase):\n",
    "    \"\"\"Base class for performing nearest-neighbour interpolation.\"\"\"\n",
    "        \n",
    "    def _interp(self, Xi, point):        \n",
    "        # The actual t, x values for the grid don't matter from this point \n",
    "        # onwards; so this is just a translation from wherever X was actually \n",
    "        # calculated. So WLOG assume it was around 0.\n",
    "        cg = coarse_grid((0, 0))\n",
    "        t, x = point\n",
    "        \n",
    "        # The grid point nearest :point:.\n",
    "        t_nearest = tools.round_mult(t, coarse_grid_sep.t, 'round')\n",
    "        x_nearest = tools.round_mult(x, coarse_grid_sep.x, 'round')\n",
    "        \n",
    "        # The value of :Xi: at those grid points.\n",
    "        t_n_x_n = Xi[self._index_tol(cg, (t_nearest, x_nearest))]\n",
    "\n",
    "        return t_n_x_n\n",
    "    \n",
    "    \n",
    "class FineGridPredictorMixin(_RegressorBase):\n",
    "    \"\"\"Provides the predict_single function for predictions on a fine grid.\"\"\"\n",
    "    \n",
    "    def predict_single(self, Xi, y):\n",
    "        returnval = []\n",
    "        # Translation doesn't matter at this point so WLOG the fine grid is\n",
    "        # around 0, 0. (cls._interp makes the same assumption; these assumptions\n",
    "        # must be consistent)\n",
    "        for point in fine_grid((0, 0)):\n",
    "            self._prepare(Xi)\n",
    "            returnval.append(self._interp(Xi, point))\n",
    "        return returnval\n",
    "    \n",
    "    \n",
    "class PointPredictorMixin(_RegressorBase):\n",
    "    \"\"\"Provides the predict_single function for predictions at a single point.\"\"\"\n",
    "    \n",
    "    def predict_single(self, Xi):\n",
    "        # Separate the location data and the grid data\n",
    "        t_offset = Xi[-2]\n",
    "        x_offset = Xi[-1]\n",
    "        Xi = Xi[:-2]\n",
    "        self._prepare(Xi)\n",
    "        # Wrapped in a list for consistency: this network just happens to only\n",
    "        # be trying to predict a single label.\n",
    "        return [self._interp(Xi, (t_offset * coarse_grid_sep.t, \n",
    "                                  x_offset * coarse_grid_sep.x))]\n",
    "    \n",
    "    \n",
    "class Perfect(_RegressorBase):\n",
    "    \"\"\"Regressor that cheats to always give the perfect prediction.\"\"\"\n",
    "    \n",
    "    def predict_single(self, Xi, y):\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using regressors\n",
    "\n",
    "def _use_regressor(regressor, processor):\n",
    "    \"\"\"Has a regressor make a prediction on some test data.\n",
    "    \n",
    "    The :regressor: argument should probably be an instance of\n",
    "    tf.estimator.Estimator or of _RegressorBase.\n",
    "    \n",
    "    The :processor: argument should be an processor with data, i.e. it\n",
    "    has already had its data function called on some BatchData.\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(regressor, 'use_tf'):\n",
    "        use_tf = regressor.use_tf\n",
    "    else:\n",
    "        use_tf = True\n",
    "\n",
    "    predictor = regressor.predict(input_fn=processor(use_tf),\n",
    "                                  yield_single_examples=False)\n",
    "    prediction = next(predictor)\n",
    "    prediction_with_postprocessing = processor.inverse_transform(prediction)\n",
    "    raw_X = processor.batch_data.X\n",
    "    raw_y = processor.batch_data.y\n",
    "    diff = prediction_with_postprocessing - raw_y\n",
    "    squared_error = np.square(diff)\n",
    "    result = tools.Object(prediction=prediction_with_postprocessing,\n",
    "                          X=raw_X,\n",
    "                          y=raw_y,\n",
    "                          diff=diff,\n",
    "                          average_loss=np.mean(squared_error),\n",
    "                          loss=np.sum(squared_error))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using regressors in ensembles\n",
    "\n",
    "class RegressorAverager:\n",
    "    \"\"\"Regressors that averages the results of other regressors to make its\n",
    "    prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    use_tf = False\n",
    "    \n",
    "    def __init__(self, regressors_with_pro, mask=None, **kwargs):\n",
    "        self.regressors_with_pro = regressors_with_pro\n",
    "        self.mask = None\n",
    "        if mask is None:\n",
    "            self.reset_mask()\n",
    "        else:\n",
    "            self.set_mask(mask)\n",
    "        super(RegressorAverager, self).__init__(**kwargs)\n",
    "        \n",
    "    def set_mask(self, mask):\n",
    "        assert len(mask) == len(self.regressors_with_pro)\n",
    "        self.mask = mask\n",
    "        return self  # for chaining\n",
    "        \n",
    "    def reset_mask(self):\n",
    "        self.mask = [True for _ in range(len(self.regressors_with_pro))]\n",
    "        return self  # for chaining\n",
    "        \n",
    "    def predict(self, input_fn, *args, **kwargs):\n",
    "        \"\"\"The argument :input_fn: should probably be an instance of BatchData.\n",
    "        \"\"\"\n",
    "        \n",
    "        X, y = input_fn()\n",
    "        X = X['X']\n",
    "        test_data = BatchData.from_single_data(X, y)\n",
    "        \n",
    "        returnval = tools.AddBase()\n",
    "        counter = 0\n",
    "        for (regressor, processor), mask in zip(self.regressors_with_pro, self.mask):\n",
    "            if mask:\n",
    "                counter += 1\n",
    "                if processor is None:\n",
    "                    processor = IdentityProcess()\n",
    "                processor.data(test_data)\n",
    "                returnval += _use_regressor(regressor, processor).prediction\n",
    "        returnval = returnval / counter\n",
    "        \n",
    "        while True:\n",
    "            yield returnval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing and evaluating regressors\n",
    "\n",
    "def test_regressors(regressors_with_pro, gen_raw_data, batch_size=1):\n",
    "    \"\"\"Tests a list of regressors on some test data. The regressors\n",
    "    may optionally have some preprocessing applied to their inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    test_data = BatchData.test(gen_raw_data, batch_size)\n",
    "    return test_regressors_on_batch_data(regressors_with_pro, test_data)\n",
    "\n",
    "\n",
    "def test_regressors_on_batch_data(regressors_with_pro, test_data):\n",
    "    \"\"\"As test_regressors, but takes a BatchData instance as an input \n",
    "    instead.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for regressor, processor in regressors_with_pro:\n",
    "        if processor is None:\n",
    "            processor = IdentityProcess()\n",
    "        processor.data(test_data)\n",
    "        \n",
    "        result = _use_regressor(regressor, processor)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualising the results of regressors\n",
    "\n",
    "# Only plots fine grid style stuff at the moment\n",
    "def plot_regressors(regressors_with_info, X, y):\n",
    "    \"\"\"Plots the results of some regressors using the given data.\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8 * len(regressors_with_info)))\n",
    "    \n",
    "    test_data = BatchData.from_single_data(X, y)\n",
    "    \n",
    "    regressors_with_pro = [xx[:2] for xx in regressors_with_info]\n",
    "    info = [xx[2] for xx in regressors_with_info]\n",
    "    \n",
    "    results = test_regressors_on_batch_data(regressors_with_pro, test_data)\n",
    "    \n",
    "    for i, (result, name) in enumerate(zip(results, info)):\n",
    "        ax = make_3d_ax_for_grid_plotting(fig, (len(regressors_with_info), 1, i + 1))\n",
    "        grid_plot(ax, X, 'cg', '_nolegend_')\n",
    "        grid_plot(ax, result.prediction, 'fg', name)\n",
    "        ax.legend()\n",
    "        \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
