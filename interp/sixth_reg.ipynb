{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As first_reg, but with seeking the difference between the label (i.e. true\n",
    "# value) and the prediction given by nearest point 'interpolation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run project_base.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to False to use existing weights for the neural network.\n",
    "# Set to True to continue training the weights from where we left off last time.\n",
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearInterp(FineGridPredictorMixin, BilinearInterpBase):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "class NearestInterp(FineGridPredictorMixin, NearestInterpBase):\n",
    "    pass\n",
    "\n",
    "\n",
    "class PolyInterp(FineGridPredictorMixin, PolyInterpBase):\n",
    "    pass\n",
    "        \n",
    "    \n",
    "# class BilinearDiffNearest(_RegressorBase):\n",
    "#     def predict_single(self, Xi, y):\n",
    "#         bilinear = BilinearInterp.predict_single(Xi, y)\n",
    "#         nearest = NearestInterp.predict_single(Xi, y)\n",
    "#         return [b - n for b, n in zip(bilinear, nearest)]\n",
    "            \n",
    "\n",
    "def gen_one_data_with_offset():\n",
    "    \"\"\"Essentially gen_one_data, just gives the offset separately.\"\"\"\n",
    "    \n",
    "    # Random solution to the CH equation\n",
    "    c = np.random.uniform(0, 10)\n",
    "    peakon = Peakon(c=c)\n",
    "    # Random location\n",
    "    t = np.random.uniform(0, 40)\n",
    "    x = c * t + np.random.uniform(-3, 3)\n",
    "    \n",
    "    # Grids at the location\n",
    "    cg = coarse_grid((t, x))\n",
    "    fg = fine_grid((t, x))\n",
    "    # Features: the solution on the coarse grid\n",
    "    X = peakon.on_grid(cg)\n",
    "    # Labels: the difference between the solution on the fine grid and the\n",
    "    # prediction given by nearest neighbour interpolation\n",
    "    y = peakon.on_grid(fg)\n",
    "    # Predict expects to be given a batch of elements, so we have to wrap\n",
    "    # the input X, y in a list, and then extract the prediction from a list\n",
    "    # of one element.\n",
    "    predictor = NearestInterp().predict(input_fn=lambda: ({'X': [X]}, [y]))\n",
    "    y_prediction = next(predictor)[0]\n",
    "    \n",
    "    return X, y, y_prediction\n",
    "    \n",
    "\n",
    "def gen_one_data():\n",
    "    \"\"\"Generates a random feature/label combination.\n",
    "    \n",
    "    Here the feature is the solution on a coarse grid and the label is the \n",
    "    difference between solution on a fine grid and the prediction given by\n",
    "    bilinear interpolation.\n",
    "    \"\"\"\n",
    "    X, y, y_prediction = gen_one_data_with_offset()\n",
    "    return X, y - y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './saved_models/sixth_reg/1000x10_121_D04', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7e1cb6e7b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "### DNN hyperparameters\n",
    "hidden_units = [1000] * 10\n",
    "# i.e. the number of fine grid points.\n",
    "logits = (fine_grid_fineness.t + 1) * (fine_grid_fineness.x + 1)\n",
    "drop_rate = 0.4\n",
    "\n",
    "### Training hyperparameters #1\n",
    "gradient_clip = 5.0  # May be set to None to turn off gradient clipping\n",
    "\n",
    "### Logging/saving hyperparameters\n",
    "log_steps = 1000  # How many steps to print the current loss.\n",
    "model_dir = './saved_models/sixth_reg/'\n",
    "model_dir = model_dir_str(model_dir, hidden_units, logits, drop_rate)\n",
    "\n",
    "dnn = Sequential.create_dnn(hidden_units, logits, drop_rate=drop_rate,\n",
    "                            model_dir=model_dir, log_steps=log_steps,\n",
    "                            gradient_clip=gradient_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    ### Training hyperparameters #2\n",
    "    batch_size = 50\n",
    "    steps = 10000\n",
    "    train_input_fn = BatchData(gen_one_data, batch_size)\n",
    "    dnn.train(input_fn=train_input_fn, max_steps=steps)\n",
    "    \n",
    "    batch_size = 100\n",
    "    steps = 20000\n",
    "    train_input_fn = BatchData(gen_one_data, batch_size)\n",
    "    dnn.train(input_fn=train_input_fn, max_steps=steps)\n",
    "    \n",
    "    batch_size = 1000\n",
    "    steps = 25000\n",
    "    train_input_fn = BatchData(gen_one_data, batch_size)\n",
    "    dnn.train(input_fn=train_input_fn, max_steps=steps)\n",
    "    \n",
    "    batch_size = 5000\n",
    "    steps = 35000\n",
    "    train_input_fn = BatchData(gen_one_data, batch_size)\n",
    "    dnn.train(input_fn=train_input_fn, max_steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, y_offset = gen_one_data_with_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./saved_models/sixth_reg/1000x10_121_D04/model.ckpt-32551\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "result = test_regressors_with_offset([(dnn, True), \n",
    "                                      (BilinearInterp(), False), \n",
    "                                      (PolyInterp(5), False)], \n",
    "                                     lambda: (X, y, y_offset), \n",
    "                                    # batch_size=100\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3222324901226659, 0.03241407376533531, 0.0020742350714215443)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].loss, result[1].loss, result[2].loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1049bd9f089495bb57302b7bd7e83b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./saved_models/sixth_reg/1000x10_121_D04/model.ckpt-32551\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "poly_deg = 5\n",
    "plot_regressors_with_offset([(Perfect(), False, 'Exact'),\n",
    "                             (dnn, True, 'DNN'),\n",
    "                             (BilinearInterp(), False, 'Bilinear'),\n",
    "                             (PolyInterp(poly_deg), False, 'Poly-{}'.format(poly_deg))],\n",
    "                            X, y, y_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
